{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "news_raw = pd.read_csv(\"C:/projectnasdaq/news_raw.csv\", encoding='latin1') # 뉴스 raw data csv파일\n",
    "nasdaq_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks.csv\", encoding='latin1') # stock_list data csv파일\n",
    "rep_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks_refine_total.csv\", encoding='latin1') # 대표단어 csv로 정리된 csv파일\n",
    "\n",
    "# 테스트 용도로 데이터 1000개\n",
    "news_raw = news_raw.head(1000)\n",
    "news_raw = news_raw[['news_id','title','summary']]\n",
    "nasdaq_stock = nasdaq_stock[['pk','symbol','name']]\n",
    "rep_stock = rep_stock[['pk','symbol','name','name_a']]\n",
    "\n",
    "# new_raw 데이터 전처리\n",
    "news_raw['summary'] = news_raw['summary'].str.lower() #소문자화\n",
    "news_raw['summary'] = news_raw['summary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip()) # news_raw summary 부분 전처리\n",
    "news_raw['summary'].replace('bloomberg', '', regex=True, inplace=True) # 불용어 지정하지 않고 bloomberg만 제거\n",
    "\n",
    "# news_raw 데이터 토큰화\n",
    "news_raw['tokenize'] = 0\n",
    "news_raw['tokenize'] = news_raw['summary'].str.split(\" \")\n",
    "news_raw['tokenize'] = news_raw['tokenize'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# news raw 데이터에서 필요한 컬럼 생성 - 단복수 처리 후 키워드 저장을 위한 컬럼\n",
    "news_raw[['lemma_summary','lemma_tokenize','news_keyword_5','news_keyword_10']] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 회사 이름 추출 => 추출 완료 ( company_word.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 회사이름 추출 데이터 저장용도\n",
    "# list에 담아주기\n",
    "tokenize_list = news_raw['tokenize']\n",
    "rep_list = rep_stock['name_a'].str.split(\",\")\n",
    "\n",
    "company_word = pd.DataFrame(columns=['news_id', 'company_word','pk']) # 추출한 회사명 용도 데이터프레임 따로 생성\n",
    "\n",
    "# 대표단어 csv파일에서 대표단어가 포함되면 맵핑\n",
    "i = 0\n",
    "\n",
    "for token_num, token in enumerate(tokenize_list):  # news_raw data 토큰화\n",
    "    for rep_num, rep in enumerate(rep_list):  # 대표단어 csv파일에서 대표단어에 해당\n",
    "        i = i + 1\n",
    "        if len(rep) == 1:\n",
    "            if rep[0] in token:\n",
    "                company_word.loc[i] = [news_raw['news_id'][token_num], rep[0], rep_stock['pk'][rep_num]]\n",
    "\n",
    "        elif len(rep) == 2:  # 대표단어가 2개로 된 단어일 때\n",
    "            if rep[0] in token:  # 대표단어의 첫번째 단어가 org단어에 있으면\n",
    "                found = token.index(rep[0])  # 대표단어의 첫번째 단어와 일치하는 org단어의 인덱스 위치 번호\n",
    "                try:\n",
    "                    search = found + 1  # stocklist의 첫번째 단어가 org에 포함됐을때 그 다음 단어\n",
    "                    search_found = token[search]  # org의 (+1을 한) 다음 단어에 해당\n",
    "                    if rep[1] == search_found:\n",
    "                        company_word.loc[i] = [news_raw['news_id'][token_num], rep[0] + \" \" + rep[1],\n",
    "                                                 rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        elif len(rep) == 3:\n",
    "            if rep[0] in token:\n",
    "                try:\n",
    "                    search_found = token[token.index(rep[0]) + 1]\n",
    "                    if rep[1] == search_found:\n",
    "                        two_found = token[token.index(rep[0]) + 2]\n",
    "                        if rep[2] == two_found:\n",
    "                            company_word.loc[i] = [news_raw['news_id'][token_num],\n",
    "                                                     rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "company_word = pd.merge(company_word, nasdaq_stock, how='left', left_on='pk', right_on='pk')\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "('bb', 'a', 'b')"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "nums = ['aa','a','aaa','a','b','b','bb','bb','bb']\n",
    "list(zip(*collections.Counter(nums).most_common(3)))[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, heapify\n",
    "from collections import Counter\n",
    "\n",
    "def solution(array, K):\n",
    "    freqs = Counter(array)\n",
    "    heap = []\n",
    "    for element, freq in freqs.items():\n",
    "        heappush(heap, (-freq,element))\n",
    "\n",
    "    result = []\n",
    "    for _ in range(K):\n",
    "        result.append(heappop(heap)[1])\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "solution(array=['aa','a','aaa','a','b','b','bb','bb','bb'], K=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 원문에서 키워드 추출 ( Keybert 사용 )=> 추출 완료 ( news_raw.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer 패키지로 단복수 문제 처리\n",
    "token = news_raw['tokenize']\n",
    "\n",
    "for i, j in enumerate(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    news_raw['lemma_tokenize'][i] = lemma_word # 단복수 처리한 토큰들\n",
    "    news_raw['lemma_summary'][i] = \" \".join(lemma_word) # 단복수한 토큰들을 join - 단복수 처리가 된 뉴스 원문 생성 용도"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# 뉴스 원문 키워드 추출\n",
    "# 단복수 처리한 뉴스 요약 내용에서 키워드 추출하기\n",
    "lemma_summary = news_raw['lemma_summary']\n",
    "for sum_num, summary in enumerate(lemma_summary):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['news_keyword_5'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))\n",
    "    news_raw['news_keyword_10'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1), top_n= 10)\n",
    "#news_raw.to_csv('C:/projectnasdaq/project_test/news_raw.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목 정보 수집 ( yfinance 패키지 사용 ) => 수집완료"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 종목코드 리스트\n",
    "stocks = nasdaq_stock['symbol']\n",
    "# 종목코드에 대한 정보 수집용 데이터프레임 생성\n",
    "stock_info = pd.DataFrame(columns=['symbol', 'info'])\n",
    "i = 0\n",
    "\n",
    "for stock_num, stock in enumerate(stocks):\n",
    "    tickers = yf.Ticker(stock)\n",
    "    ticker = tickers.info\n",
    "    i = i + 1\n",
    "    #print(stock_num,stock, \"===>>\",ticker)\n",
    "    stock_info.loc[i] = [stock, ticker]\n",
    "    time.sleep(random.uniform(3, 4))\n",
    "\n",
    "#stock_info.to_csv('C:/projectnasdaq/project2/stock_info.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yahoo_dataset 파일 수정 ( yahoo_dataset_mapping.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 파일 수정 ( -> symbol name 수정 )\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset.csv')\n",
    "\n",
    "# yahoo_dataset에서 필요없는 컬럼들 지우고 csv 파일에 있는 종목코드에 종목명 데이터들 붙임\n",
    "yahoo_dataset.insert(1,'new_symbol','0')\n",
    "yahoo_dataset.insert(2,'name','0')\n",
    "yahoo_dataset.insert(3,'company_word','0')\n",
    "\n",
    "yahoo_dataset_symbol = yahoo_dataset['symbol']\n",
    "rep_stock_symbol = rep_stock['symbol']\n",
    "\n",
    "for i, j in enumerate(yahoo_dataset_symbol):\n",
    "    for k, l in enumerate(rep_stock_symbol):\n",
    "        if j == l:\n",
    "            yahoo_dataset['new_symbol'][i] = rep_stock['symbol'][k]\n",
    "            yahoo_dataset['name'][i] = rep_stock['name'][k]\n",
    "            yahoo_dataset['company_word'][i] = rep_stock['name_a'][k]\n",
    "            break\n",
    "\n",
    "# yahoo_dataset 컬럼 정리\n",
    "yahoo_dataset.drop(columns=[\"quoteType\", \"currency\", 'regularMarketPrice', 'regularMarketChange', 'regularMarketChangePercent',\n",
    "             'regularMarketVolume', 'averageDailyVolume3Month', 'marketCap', 'trailingPE', 'fiftyTwoWeekLow',\n",
    "             'fiftyTwoWeekHigh', 'regularMarketOpen', 'priceHint', 'underlyingSymbol'], inplace=True)\n",
    "#yahoo_dataset.to_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추출된 회사 이름이랑 ( 전처리 한 ) 뉴스 키워드 연결 ( company_word.csv 파일에 저장 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/news_2000/company_word.csv')\n",
    "news_raw_keyword = pd.read_csv('C:/projectnasdaq/project_test/news_2000/news_raw.csv')\n",
    "# 실행을 전부 돌리면 시간 오래 걸려서 미리 키워드 뽑아놓고 계속 파일 가져와서 썼기때문에 news_raw_keyword라는 이름으로 가져와서 씀\n",
    "# news_raw 이름으로 그대로 가져오면 키워드 컬럼이 비워져있는 상태라서 오류 발생\n",
    "\n",
    "# 키워드 전처리 -> 키워드가  ex) ( apple,0.1566) 이런식으로 추출이 돼서 단어만 뽑기 위해 전처리\n",
    "# 뉴스 원문의 키워드 5개 -> 종목명의 대표 키워드를 만드는 용도\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].str.split(\" \")\n",
    "\n",
    "# 뉴스 원문의 키워드 10개 ]\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].str.split(\" \")\n",
    "\n",
    "# 뉴스 원문에서 추출된 종목명의 뉴스 키워드 저장용 컬럼\n",
    "company_word['stock_keyword_5'] = 0\n",
    "\n",
    "keyword_list = news_raw_keyword['news_keyword_5']\n",
    "\n",
    "news_raw_id = news_raw_keyword['news_id']\n",
    "company_word_id = company_word['news_id']\n",
    "\n",
    "for i, j in enumerate(company_word_id):\n",
    "    for k, l in enumerate(news_raw_id):\n",
    "        if j == l:\n",
    "            company_word['stock_keyword_5'][i] = news_raw_keyword['news_keyword_5'][k]\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목명이 추출된 뉴스 키워드들끼리 모으기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# company 단어들 겹치는 거 없이 하나씩만 list에 담기 -> 단어 이름이 같은 종목명들의 키워드 모으기 위해서\n",
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/news_2000/company_word.csv') # 뉴스원문에서 추출된 회사명과 회사명이 속한 뉴스의 키워드가 저장된 csv파일\n",
    "company_words = company_word['company_word']\n",
    "\n",
    "company_word_same_list = []\n",
    "for company in company_words:\n",
    "    if company not in company_word_same_list:\n",
    "        company_word_same_list.append(company)\n",
    "\n",
    "company_word_sames = pd.DataFrame(company_word_same_list, columns=['company_word_same'])\n",
    "company_word_same = company_word_sames['company_word_same']\n",
    "\n",
    "# 키워드랑 company_name이랑 데이터프레임에서 맵핑 시키기위한 새로운 dataframe 생성\n",
    "stock_info = pd.DataFrame(columns=['company', 'stock_keyword'])\n",
    "\n",
    "# 회사명이 추출된 뉴스 원문의 키워드들을 전부 리스트 형태로 모으기\n",
    "for i, j in enumerate(company_word_same):\n",
    "    stock_info.loc[i] = [j, company_word[company_word['company_word'] == j]['stock_keyword_5'].tolist()]\n",
    "\n",
    "# company_name_keyword_organize.csv에 column 삽입\n",
    "stock_info.insert(0, 'symbol', '0')\n",
    "stock_info.insert(1, 'name', '0')\n",
    "\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "company_word_ = company_word['company_word']\n",
    "company = stock_info['company']\n",
    "\n",
    "# stock_info에 symbol, name 부분 붙이기\n",
    "for i,j in enumerate(company_word_):\n",
    "    for k, l in enumerate(company):\n",
    "        if j==l:\n",
    "            stock_info['symbol'][k] = company_word['symbol'][i]\n",
    "            stock_info['name'][k] = company_word['name'][i]\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 키워드 리스트를 하나의 리스트로 합치기\n",
    "stock_keyword = stock_info['stock_keyword']\n",
    "for i, j in enumerate(stock_keyword):\n",
    "    stock_info['stock_keyword'][i] = ''.join(j)\n",
    "\n",
    "# 합친 키워드 리스트 전처리\n",
    "stock_info['stock_keyword'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'].replace(',',' ',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].str.split(\" \")\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# 키워드들 높은 빈도순으로 추출 한 데이터 저장 컬럼 생성\n",
    "stock_info[['stock_frequency_keyword_4','stock_frequency_keyword_5']] = 0\n",
    "frequency_word = stock_info['stock_keyword']\n",
    "\n",
    "for i, j in enumerate(frequency_word):\n",
    "    # 대표 키워드 단어 빈도 순으로 4개 추출\n",
    "    stock_info['stock_frequency_keyword_4'][i] = Counter(j).most_common(4)\n",
    "#    # 대표 키워드 단어 빈도 순으로 5개 추출\n",
    "    stock_info['stock_frequency_keyword_5'][i] = Counter(j).most_common(5)\n",
    "\n",
    "# 4개 뽑아온 키워드 전처리 -> 프로젝트 진행하면서 키워드 4개짜리는 사용 안함\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# 종목명에 대한 뉴스 keyword가 15개 이하면 공백처리\n",
    "stock_keyword = stock_info['stock_keyword']\n",
    "\n",
    "for i,j in enumerate(stock_keyword):\n",
    "    number = set(j)\n",
    "    if len(number) <= 15:\n",
    "        stock_info['stock_frequency_keyword_5'][i] = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) 뉴스 원문에 종목명의 대표 키워드가 포함될 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문에 대표 키워드가 포함될 경우 317\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑(lemmatize)\n",
    "lemma_tokenize_list = news_raw['lemma_tokenize']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "mapping_result = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(lemma_tokenize_list):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 5개가 교집합으로 있을 경우\n",
    "        if len(together)==5:\n",
    "            mapping_result.loc[a] = [news_raw['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k]]\n",
    "\n",
    "print('뉴스 원문에 대표 키워드가 포함될 경우',len(mapping_result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 : 947\n"
     ]
    }
   ],
   "source": [
    "mapping_result_2 = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5','news_keyword_10'])\n",
    "news_keyword_10 = news_raw_keyword['news_keyword_10']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_keyword_10):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            mapping_result_2.loc[a] = [news_raw_keyword['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw_keyword['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k],news_raw_keyword['news_keyword_10'][i]]\n",
    "\n",
    "print('뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 :',len(mapping_result_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "       news_id symbol                                       name  \\\n8161    345638   MSFT                      Microsoft Corporation   \n10021   345667    UAL               United Airlines Holdings Inc   \n11881   347821    CVX                        Chevron Corporation   \n11898   347821    PBT                Permian Basin Royalty Trust   \n18408   353874    MRK                        Merck & Company Inc   \n...        ...    ...                                        ...   \n612179  443702   MSCI                                   MSCI Inc   \n612184  443702   AAPL                                  Apple Inc   \n612211  443702   APWC  Asia Pacific Wire & Cable Corporation Ltd   \n615337  443718    NMR                    Nomura Holdings Inc ADR   \n620929  443773   MSFT                      Microsoft Corporation   \n\n                                            lemma_summary  \\\n8161    stock bull have taken over emerging market a t...   \n10021   united airline holding inc warned that it s co...   \n11881   wall street expected exxon mobil corp and chev...   \n11898   wall street expected exxon mobil corp and chev...   \n18408   the s&p 500 index posted a fourth straight adv...   \n...                                                   ...   \n612179  stock slumped to an eight week low amid warnin...   \n612184  stock slumped to an eight week low amid warnin...   \n612211  stock slumped to an eight week low amid warnin...   \n615337  new zealand s central bank said it may launch ...   \n620929  localiza rent a car sa and unidas two of brazi...   \n\n                               stock_frequency_keyword_5  \n8161              [stock, pandemic, market, bank, covid]  \n10021   [airline, covid, coronavirus, furlough, carrier]  \n11881            [petroleum, shale, stock, oil, chevron]  \n11898          [shale, petroleum, stock, exxon, chevron]  \n18408             [investor, stock, covid, equity, euro]  \n...                                                  ...  \n612179    [stock, stocksthe, investor, pandemic, market]  \n612184      [stock, nasdaq, stocksthe, market, investor]  \n612211  [stock, stocksthe, pandemic, treasury, investor]  \n615337     [bank, monetary, easing, market, coronavirus]  \n620929            [stock, pandemic, market, bank, covid]  \n\n[317 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>lemma_summary</th>\n      <th>stock_frequency_keyword_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8161</th>\n      <td>345638</td>\n      <td>MSFT</td>\n      <td>Microsoft Corporation</td>\n      <td>stock bull have taken over emerging market a t...</td>\n      <td>[stock, pandemic, market, bank, covid]</td>\n    </tr>\n    <tr>\n      <th>10021</th>\n      <td>345667</td>\n      <td>UAL</td>\n      <td>United Airlines Holdings Inc</td>\n      <td>united airline holding inc warned that it s co...</td>\n      <td>[airline, covid, coronavirus, furlough, carrier]</td>\n    </tr>\n    <tr>\n      <th>11881</th>\n      <td>347821</td>\n      <td>CVX</td>\n      <td>Chevron Corporation</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n      <td>[petroleum, shale, stock, oil, chevron]</td>\n    </tr>\n    <tr>\n      <th>11898</th>\n      <td>347821</td>\n      <td>PBT</td>\n      <td>Permian Basin Royalty Trust</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n      <td>[shale, petroleum, stock, exxon, chevron]</td>\n    </tr>\n    <tr>\n      <th>18408</th>\n      <td>353874</td>\n      <td>MRK</td>\n      <td>Merck &amp; Company Inc</td>\n      <td>the s&amp;p 500 index posted a fourth straight adv...</td>\n      <td>[investor, stock, covid, equity, euro]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>612179</th>\n      <td>443702</td>\n      <td>MSCI</td>\n      <td>MSCI Inc</td>\n      <td>stock slumped to an eight week low amid warnin...</td>\n      <td>[stock, stocksthe, investor, pandemic, market]</td>\n    </tr>\n    <tr>\n      <th>612184</th>\n      <td>443702</td>\n      <td>AAPL</td>\n      <td>Apple Inc</td>\n      <td>stock slumped to an eight week low amid warnin...</td>\n      <td>[stock, nasdaq, stocksthe, market, investor]</td>\n    </tr>\n    <tr>\n      <th>612211</th>\n      <td>443702</td>\n      <td>APWC</td>\n      <td>Asia Pacific Wire &amp; Cable Corporation Ltd</td>\n      <td>stock slumped to an eight week low amid warnin...</td>\n      <td>[stock, stocksthe, pandemic, treasury, investor]</td>\n    </tr>\n    <tr>\n      <th>615337</th>\n      <td>443718</td>\n      <td>NMR</td>\n      <td>Nomura Holdings Inc ADR</td>\n      <td>new zealand s central bank said it may launch ...</td>\n      <td>[bank, monetary, easing, market, coronavirus]</td>\n    </tr>\n    <tr>\n      <th>620929</th>\n      <td>443773</td>\n      <td>MSFT</td>\n      <td>Microsoft Corporation</td>\n      <td>localiza rent a car sa and unidas two of brazi...</td>\n      <td>[stock, pandemic, market, bank, covid]</td>\n    </tr>\n  </tbody>\n</table>\n<p>317 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "        news_id symbol                                               name  \\\n1        345585     KO                            Coca-Cola Company (The)   \n2        345585     UL                                       Unilever PLC   \n3        345585    DEO                                         Diageo plc   \n4        345585    BUD    Anheuser-Busch Inbev SA Sponsored ADR (Belgium)   \n5        345585   SBUX                              Starbucks Corporation   \n...         ...    ...                                                ...   \n1241786  366894    DOW                                            Dow Inc   \n1241799  366894     AU                          AngloGold Ashanti Limited   \n1241877  366894    FIS         Fidelity National Information Services Inc   \n1241906  366894   ASGI  Aberdeen Standard Global Infrastructure Income Fd   \n1246167  366932     AU                          AngloGold Ashanti Limited   \n\n                                             lemma_summary  \\\n1        nestle sa sailed past ailing consumer good riv...   \n2        nestle sa sailed past ailing consumer good riv...   \n3        nestle sa sailed past ailing consumer good riv...   \n4        nestle sa sailed past ailing consumer good riv...   \n5        nestle sa sailed past ailing consumer good riv...   \n...                                                    ...   \n1241786  u s stock turned sharply lower in the final ho...   \n1241799  u s stock turned sharply lower in the final ho...   \n1241877  u s stock turned sharply lower in the final ho...   \n1241906  u s stock turned sharply lower in the final ho...   \n1246167  the latest market rout in turkish asset erased...   \n\n                                 stock_frequency_keyword_5  \\\n1              [cola, pepsico, nestle, starbucks, bottled]   \n2        [market, nestle, starbucks, bottled, coladefor...   \n3         [nestle, starbucks, bottled, market, coladiageo]   \n4        [nestle, starbucks, bottled, market, colainves...   \n5           [starbucks, coffee, shenzhen, nestle, bottled]   \n...                                                    ...   \n1241786           [dow, stock, market, pandemic, investor]   \n1241799            [market, rand, investor, stock, trader]   \n1241877    [investment, market, investor, currency, stock]   \n1241906     [investor, investment, market, stock, chinese]   \n1246167            [market, rand, investor, stock, trader]   \n\n                                           news_keyword_10  \n1        [nestle, starbucks, bottled, market, cola, nes...  \n2        [nestle, starbucks, bottled, market, cola, nes...  \n3        [nestle, starbucks, bottled, market, cola, nes...  \n4        [nestle, starbucks, bottled, market, cola, nes...  \n5        [nestle, starbucks, bottled, market, cola, nes...  \n...                                                    ...  \n1241786  [rally, market, stock, dow, investor, trading,...  \n1241799  [rally, market, stock, dow, investor, trading,...  \n1241877  [rally, market, stock, dow, investor, trading,...  \n1241906  [rally, market, stock, dow, investor, trading,...  \n1246167  [market, trader, istanbul, turkish, investor, ...  \n\n[947 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>lemma_summary</th>\n      <th>stock_frequency_keyword_5</th>\n      <th>news_keyword_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>345585</td>\n      <td>KO</td>\n      <td>Coca-Cola Company (The)</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[cola, pepsico, nestle, starbucks, bottled]</td>\n      <td>[nestle, starbucks, bottled, market, cola, nes...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345585</td>\n      <td>UL</td>\n      <td>Unilever PLC</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[market, nestle, starbucks, bottled, coladefor...</td>\n      <td>[nestle, starbucks, bottled, market, cola, nes...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>345585</td>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[nestle, starbucks, bottled, market, coladiageo]</td>\n      <td>[nestle, starbucks, bottled, market, cola, nes...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>345585</td>\n      <td>BUD</td>\n      <td>Anheuser-Busch Inbev SA Sponsored ADR (Belgium)</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[nestle, starbucks, bottled, market, colainves...</td>\n      <td>[nestle, starbucks, bottled, market, cola, nes...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>345585</td>\n      <td>SBUX</td>\n      <td>Starbucks Corporation</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[starbucks, coffee, shenzhen, nestle, bottled]</td>\n      <td>[nestle, starbucks, bottled, market, cola, nes...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1241786</th>\n      <td>366894</td>\n      <td>DOW</td>\n      <td>Dow Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[dow, stock, market, pandemic, investor]</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1241799</th>\n      <td>366894</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[market, rand, investor, stock, trader]</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1241877</th>\n      <td>366894</td>\n      <td>FIS</td>\n      <td>Fidelity National Information Services Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[investment, market, investor, currency, stock]</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1241906</th>\n      <td>366894</td>\n      <td>ASGI</td>\n      <td>Aberdeen Standard Global Infrastructure Income Fd</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[investor, investment, market, stock, chinese]</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1246167</th>\n      <td>366932</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>the latest market rout in turkish asset erased...</td>\n      <td>[market, rand, investor, stock, trader]</td>\n      <td>[market, trader, istanbul, turkish, investor, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>947 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_result_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "     news_id symbol                                               name  \\\n0     345638   MSFT                              Microsoft Corporation   \n1     345667    UAL                       United Airlines Holdings Inc   \n2     347821    CVX                                Chevron Corporation   \n3     347821    PBT                        Permian Basin Royalty Trust   \n4     353874    MRK                                Merck & Company Inc   \n...      ...    ...                                                ...   \n1259  366894    DOW                                            Dow Inc   \n1260  366894     AU                          AngloGold Ashanti Limited   \n1261  366894    FIS         Fidelity National Information Services Inc   \n1262  366894   ASGI  Aberdeen Standard Global Infrastructure Income Fd   \n1263  366932     AU                          AngloGold Ashanti Limited   \n\n                                          lemma_summary  \n0     stock bull have taken over emerging market a t...  \n1     united airline holding inc warned that it s co...  \n2     wall street expected exxon mobil corp and chev...  \n3     wall street expected exxon mobil corp and chev...  \n4     the s&p 500 index posted a fourth straight adv...  \n...                                                 ...  \n1259  u s stock turned sharply lower in the final ho...  \n1260  u s stock turned sharply lower in the final ho...  \n1261  u s stock turned sharply lower in the final ho...  \n1262  u s stock turned sharply lower in the final ho...  \n1263  the latest market rout in turkish asset erased...  \n\n[1115 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>lemma_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>345638</td>\n      <td>MSFT</td>\n      <td>Microsoft Corporation</td>\n      <td>stock bull have taken over emerging market a t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>345667</td>\n      <td>UAL</td>\n      <td>United Airlines Holdings Inc</td>\n      <td>united airline holding inc warned that it s co...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>347821</td>\n      <td>CVX</td>\n      <td>Chevron Corporation</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>347821</td>\n      <td>PBT</td>\n      <td>Permian Basin Royalty Trust</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>353874</td>\n      <td>MRK</td>\n      <td>Merck &amp; Company Inc</td>\n      <td>the s&amp;p 500 index posted a fourth straight adv...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1259</th>\n      <td>366894</td>\n      <td>DOW</td>\n      <td>Dow Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n    </tr>\n    <tr>\n      <th>1260</th>\n      <td>366894</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n    </tr>\n    <tr>\n      <th>1261</th>\n      <td>366894</td>\n      <td>FIS</td>\n      <td>Fidelity National Information Services Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n    </tr>\n    <tr>\n      <th>1262</th>\n      <td>366894</td>\n      <td>ASGI</td>\n      <td>Aberdeen Standard Global Infrastructure Income Fd</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n    </tr>\n    <tr>\n      <th>1263</th>\n      <td>366932</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>the latest market rout in turkish asset erased...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1115 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 두가지 방법을 합쳤을 때 중복 행 제거\n",
    "mapping_result = mapping_result.drop(columns=['stock_frequency_keyword_5'])\n",
    "mapping_result_2 = mapping_result_2.drop(columns=['stock_frequency_keyword_5', 'news_keyword_10'])\n",
    "mapping_result_final = pd.concat([mapping_result,mapping_result_2], ignore_index=True)\n",
    "overlap = mapping_result_final.drop_duplicates(['news_id','symbol'])\n",
    "overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "overlap = pd.read_csv('C:/projectnasdaq/project_test/news_2000/overlap_remove.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "0       stock bull have taken over emerging market a t...\n1       united airline holding inc warned that it s co...\n2       wall street expected exxon mobil corp and chev...\n3       wall street expected exxon mobil corp and chev...\n4       the s&p 500 index posted a fourth straight adv...\n                              ...                        \n1110    u s stock turned sharply lower in the final ho...\n1111    u s stock turned sharply lower in the final ho...\n1112    u s stock turned sharply lower in the final ho...\n1113    u s stock turned sharply lower in the final ho...\n1114    the latest market rout in turkish asset erased...\nName: lemma_summary, Length: 1115, dtype: object"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_summary = news_raw_keyword['lemma_summary']\n",
    "overlap_summary = overlap['lemma_summary']\n",
    "overlap_summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "news_summary = news_raw_keyword['lemma_summary']\n",
    "overlap_summary = overlap['lemma_summary']\n",
    "overlap['news_keyword_10'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "for k,l in enumerate(overlap_summary):\n",
    "    for i,j in enumerate(news_summary):\n",
    "        if l==j:\n",
    "            overlap['news_keyword_10'][k] = news_raw_keyword['news_keyword_10'][i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0  news_id symbol  \\\n0              0   345638   MSFT   \n1              1   345667    UAL   \n2              2   347821    CVX   \n3              3   347821    PBT   \n4              4   353874    MRK   \n...          ...      ...    ...   \n1110        1259   366894    DOW   \n1111        1260   366894     AU   \n1112        1261   366894    FIS   \n1113        1262   366894   ASGI   \n1114        1263   366932     AU   \n\n                                                   name  \\\n0                                 Microsoft Corporation   \n1                          United Airlines Holdings Inc   \n2                                   Chevron Corporation   \n3                           Permian Basin Royalty Trust   \n4                                   Merck & Company Inc   \n...                                                 ...   \n1110                                            Dow Inc   \n1111                          AngloGold Ashanti Limited   \n1112         Fidelity National Information Services Inc   \n1113  Aberdeen Standard Global Infrastructure Income Fd   \n1114                          AngloGold Ashanti Limited   \n\n                                          lemma_summary  \\\n0     stock bull have taken over emerging market a t...   \n1     united airline holding inc warned that it s co...   \n2     wall street expected exxon mobil corp and chev...   \n3     wall street expected exxon mobil corp and chev...   \n4     the s&p 500 index posted a fourth straight adv...   \n...                                                 ...   \n1110  u s stock turned sharply lower in the final ho...   \n1111  u s stock turned sharply lower in the final ho...   \n1112  u s stock turned sharply lower in the final ho...   \n1113  u s stock turned sharply lower in the final ho...   \n1114  the latest market rout in turkish asset erased...   \n\n                                        news_keyword_10  \n0     [stock, emerging, investment, market, bullish,...  \n1     [furlough, airline, carrier, covid, united, ai...  \n2     [exxon, petroleum, chevron, shale, earnings, l...  \n3     [exxon, petroleum, chevron, shale, earnings, l...  \n4     [stock, stocksthe, investor, trader, market, t...  \n...                                                 ...  \n1110  [rally, market, stock, dow, investor, trading,...  \n1111  [rally, market, stock, dow, investor, trading,...  \n1112  [rally, market, stock, dow, investor, trading,...  \n1113  [rally, market, stock, dow, investor, trading,...  \n1114  [market, trader, istanbul, turkish, investor, ...  \n\n[1115 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>lemma_summary</th>\n      <th>news_keyword_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>345638</td>\n      <td>MSFT</td>\n      <td>Microsoft Corporation</td>\n      <td>stock bull have taken over emerging market a t...</td>\n      <td>[stock, emerging, investment, market, bullish,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>345667</td>\n      <td>UAL</td>\n      <td>United Airlines Holdings Inc</td>\n      <td>united airline holding inc warned that it s co...</td>\n      <td>[furlough, airline, carrier, covid, united, ai...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>347821</td>\n      <td>CVX</td>\n      <td>Chevron Corporation</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n      <td>[exxon, petroleum, chevron, shale, earnings, l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>347821</td>\n      <td>PBT</td>\n      <td>Permian Basin Royalty Trust</td>\n      <td>wall street expected exxon mobil corp and chev...</td>\n      <td>[exxon, petroleum, chevron, shale, earnings, l...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>353874</td>\n      <td>MRK</td>\n      <td>Merck &amp; Company Inc</td>\n      <td>the s&amp;p 500 index posted a fourth straight adv...</td>\n      <td>[stock, stocksthe, investor, trader, market, t...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1110</th>\n      <td>1259</td>\n      <td>366894</td>\n      <td>DOW</td>\n      <td>Dow Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1111</th>\n      <td>1260</td>\n      <td>366894</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1112</th>\n      <td>1261</td>\n      <td>366894</td>\n      <td>FIS</td>\n      <td>Fidelity National Information Services Inc</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1113</th>\n      <td>1262</td>\n      <td>366894</td>\n      <td>ASGI</td>\n      <td>Aberdeen Standard Global Infrastructure Income Fd</td>\n      <td>u s stock turned sharply lower in the final ho...</td>\n      <td>[rally, market, stock, dow, investor, trading,...</td>\n    </tr>\n    <tr>\n      <th>1114</th>\n      <td>1263</td>\n      <td>366932</td>\n      <td>AU</td>\n      <td>AngloGold Ashanti Limited</td>\n      <td>the latest market rout in turkish asset erased...</td>\n      <td>[market, trader, istanbul, turkish, investor, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1115 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "#overlap.to_csv('C:/projectnasdaq/project_test/news_2000/overlap_remove.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 에서 종목 정보로 뉴스 원문과 매핑하기\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "yahoo_dataset[['longBusinessSummary','longBusinessSummary_keyword']] = 0\n",
    "yahoo_dataset['company_info'] = yahoo_dataset['company_info'].fillna(\"0\")\n",
    "company_info = yahoo_dataset['company_info']\n",
    "\n",
    "# yfinance company_info에서 longBusinessSummary 부분 추출\n",
    "for i,j in enumerate(company_info):\n",
    "    keys =[]\n",
    "    values = []\n",
    "    company_info_list = j.split(', \"')\n",
    "    for info_list in company_info_list:\n",
    "        pair = info_list.split('\":')\n",
    "        try:\n",
    "            keys.append(pair[0])\n",
    "            values.append(pair[1])\n",
    "            my_dict = dict(zip(keys, values))\n",
    "            BusinessSummary= my_dict.get('longBusinessSummary')\n",
    "            yahoo_dataset['longBusinessSummary'][i] = BusinessSummary\n",
    "        except IndexError: continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# longBusinessSummary column 소문자화\n",
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].str.lower() #소문자화\n",
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "# shortName column 소문자화\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].str.lower() #소문자화\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "long_business_summary = yahoo_dataset['longBusinessSummary']\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].fillna(\"\")\n",
    "shortname = yahoo_dataset['shortName']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 종목명 설명 컬럼에 종목명이 포함되면 지우기\n",
    "for i,j in enumerate(long_business_summary):\n",
    "    for k,l in enumerate(shortname):\n",
    "        if l in j:\n",
    "            yahoo_dataset['longBusinessSummary'][i] = yahoo_dataset['longBusinessSummary'][k].replace(l,'')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "long_business_summary_stopwords = {'company','corp','inc'}\n",
    "yahoo_dataset['longBusinessSummary'].replace(long_business_summary_stopwords, '', regex=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# longBusinessSummary 단복수 처리\n",
    "long_business_summary_token = yahoo_dataset['longBusinessSummary'].str.split(\" \")\n",
    "long_business_summary_token = long_business_summary_token.apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "for i,j in enumerate(long_business_summary_token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    yahoo_dataset['longBusinessSummary'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3568\\966386875.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mlong_business_summary\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0myahoo_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'longBusinessSummary'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msum_num\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummary\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlong_business_summary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0myahoo_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'longBusinessSummary_keyword'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\_model.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m     49\u001B[0m                       \u001B[1;33m*\u001B[0m \u001B[0mhttps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m//\u001B[0m\u001B[0mwww\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msbert\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mpretrained_models\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhtml\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \"\"\"\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect_backend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     def extract_keywords(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_utils.py\u001B[0m in \u001B[0;36mselect_backend\u001B[1;34m(embedding_model)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;31m# Create a Sentence Transformer model based on a string\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mSentenceTransformerBackend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;31m# Hugging Face embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, embedding_model)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0membedding_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSentenceTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m             raise ValueError(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'modules.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m#Load as SentenceTransformer model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_sbert_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m   \u001B[1;31m#Load with AutoModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_auto_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m_load_sbert_model\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule_config\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodules_config\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    839\u001B[0m             \u001B[0mmodule_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimport_from_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'type'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 840\u001B[1;33m             \u001B[0mmodule\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'path'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    841\u001B[0m             \u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    842\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(input_path)\u001B[0m\n\u001B[0;32m    135\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msbert_config_path\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfIn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m             \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfIn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtokenizer_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m_load_model\u001B[1;34m(self, model_name_or_path, config, cache_dir)\u001B[0m\n\u001B[0;32m     47\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m             \u001B[0mmodel_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_model_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mmodel_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m         raise ValueError(\n\u001B[0;32m    448\u001B[0m             \u001B[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2105\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mContextManagers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minit_contexts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2106\u001B[1;33m             \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2108\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdevice_map\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"auto\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config, add_pooling_layer)\u001B[0m\n\u001B[0;32m    885\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 887\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEmbeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    888\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEncoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    889\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mposition_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_position_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoken_type_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_vocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_weight\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfactory_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 140\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    141\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_weight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36mreset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m         \u001B[0minit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    150\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fill_padding_idx_with_zero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36mnormal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m    153\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhas_torch_function_variadic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtrunc_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m2.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2.\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36m_no_grad_normal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# longBusinessSummary 키워드 5개 추출\n",
    "long_business_summary =  yahoo_dataset['longBusinessSummary']\n",
    "for sum_num, summary in enumerate(long_business_summary):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    yahoo_dataset['longBusinessSummary_keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# 뽑아낸 keyword 전처리\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].str.split(\" \")\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}