{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "news_raw = pd.read_csv(\"C:/projectnasdaq/news_raw.csv\", encoding='latin1') # 뉴스 raw data\n",
    "nasdaq_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks.csv\", encoding='latin1') # stock_list data\n",
    "rep_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks_refine_total.csv\", encoding='latin1') # 대표단어 csv\n",
    "\n",
    "# 테스트 용도로 데이터 1000개\n",
    "news_raw = news_raw.head(1000)\n",
    "news_raw = news_raw[['news_id','title','summary']]\n",
    "nasdaq_stock = nasdaq_stock[['pk','symbol','name']]\n",
    "rep_stock = rep_stock[['pk','symbol','name','name_a']]\n",
    "\n",
    "# new_raw 데이터 전처리\n",
    "news_raw['summary'] = news_raw['summary'].str.lower() #소문자화\n",
    "news_raw['summary'] = news_raw['summary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "news_raw['summary'].replace('bloomberg', '', regex=True, inplace=True)\n",
    "\n",
    "# news_raw 데이터 토큰화\n",
    "news_raw['tokenize'] = 0\n",
    "news_raw['tokenize'] = news_raw['summary'].str.split(\" \")\n",
    "news_raw['tokenize'] = news_raw['tokenize'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# news raw 데이터에서 필요한 컬럼 생성\n",
    "news_raw[['lemma_summary','lemma_tokenize','news_keyword_5','news_keyword_10']] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 회사 이름 추출 => 추출 완료 ( company_word.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8072\\4084188830.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     38\u001B[0m                                                      rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n\u001B[0;32m     39\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m                     \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[0mcompany_word\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcompany_word\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnasdaq_stock\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'left'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mleft_on\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pk'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright_on\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pk'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 회사이름 추출 데이터 저장용도\n",
    "# list에 담아주기\n",
    "tokenize_list = news_raw['tokenize']\n",
    "rep_list = rep_stock['name_a'].str.split(\",\")\n",
    "\n",
    "company_word = pd.DataFrame(columns=['news_id', 'company_word','pk'])\n",
    "\n",
    "# 대표단어 csv파일에서 대표단어가 포함되면 맵핑\n",
    "i = 0\n",
    "\n",
    "for token_num, token in enumerate(tokenize_list):  # news_raw data 토큰화\n",
    "    for rep_num, rep in enumerate(rep_list):  # 대표단어 csv파일에서 대표단어에 해당\n",
    "        i = i + 1\n",
    "        if len(rep) == 1:\n",
    "            if rep[0] in token:\n",
    "                company_word.loc[i] = [news_raw['news_id'][token_num], rep[0], rep_stock['pk'][rep_num]]\n",
    "\n",
    "        elif len(rep) == 2:  # 대표단어가 2개로 된 단어일 때\n",
    "            if rep[0] in token:  # 대표단어의 첫번째 단어가 org단어에 있으면\n",
    "                found = token.index(rep[0])  # 대표단어의 첫번째 단어와 일치하는 org단어의 인덱스 위치 번호\n",
    "                try:\n",
    "                    search = found + 1  # stocklist의 첫번째 단어가 org에 포함됐을때 그 다음 단어\n",
    "                    search_found = token[search]  # org의 (+1을 한) 다음 단어에 해당\n",
    "                    if rep[1] == search_found:\n",
    "                        company_word.loc[i] = [news_raw['news_id'][token_num], rep[0] + \" \" + rep[1],\n",
    "                                                 rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        elif len(rep) == 3:\n",
    "            if rep[0] in token:\n",
    "                try:\n",
    "                    search_found = token[token.index(rep[0]) + 1]\n",
    "                    if rep[1] == search_found:\n",
    "                        two_found = token[token.index(rep[0]) + 2]\n",
    "                        if rep[2] == two_found:\n",
    "                            company_word.loc[i] = [news_raw['news_id'][token_num],\n",
    "                                                     rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "company_word = pd.merge(company_word, nasdaq_stock, how='left', left_on='pk', right_on='pk')\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 원문에서 키워드 추출 ( Keybert 사용 )=> 추출 완료 ( news_raw.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer 패키지로 단복수 문제 처리\n",
    "token = news_raw['tokenize']\n",
    "\n",
    "for i, j in enumerate(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    news_raw['lemma_tokenize'][i] = lemma_word\n",
    "    news_raw['lemma_summary'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8072\\639550318.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'news_keyword_5'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'news_keyword_10'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtop_n\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;31m#news_raw.to_csv('C:/projectnasdaq/project_test/news_raw.csv')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\_model.py\u001B[0m in \u001B[0;36mextract_keywords\u001B[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001B[0m\n\u001B[0;32m    153\u001B[0m         \u001B[1;31m# Extract embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[0mdoc_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m         \u001B[0mword_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[1;31m# Find keywords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\u001B[0m in \u001B[0;36membed\u001B[1;34m(self, documents, verbose)\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mthat\u001B[0m \u001B[0meach\u001B[0m \u001B[0mhave\u001B[0m \u001B[0man\u001B[0m \u001B[0membeddings\u001B[0m \u001B[0msize\u001B[0m \u001B[0mof\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mm\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m         \"\"\"\n\u001B[1;32m---> 62\u001B[1;33m         \u001B[0membeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocuments\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     63\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0membeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36mencode\u001B[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[0;32m    159\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mstart_index\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Batches\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mnot\u001B[0m \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m             \u001B[0msentences_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msentences_sorted\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstart_index\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstart_index\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 161\u001B[1;33m             \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    162\u001B[0m             \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_to_device\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    317\u001B[0m         \u001B[0mTokenizes\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m         \"\"\"\n\u001B[1;32m--> 319\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_first_module\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_sentence_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    111\u001B[0m             \u001B[0mto_tokenize\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcol\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mto_tokenize\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m         \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mto_tokenize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtruncation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'longest_first'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"pt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_seq_length\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2532\u001B[0m                 \u001B[0mreturn_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2533\u001B[0m                 \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2534\u001B[1;33m                 \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2535\u001B[0m             )\n\u001B[0;32m   2536\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mbatch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2723\u001B[0m             \u001B[0mreturn_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2724\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2725\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2726\u001B[0m         )\n\u001B[0;32m   2727\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001B[0m in \u001B[0;36m_batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[0;32m    427\u001B[0m             \u001B[0mbatch_text_or_text_pairs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m             \u001B[0madd_special_tokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0madd_special_tokens\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 429\u001B[1;33m             \u001B[0mis_pretokenized\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mis_split_into_words\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    430\u001B[0m         )\n\u001B[0;32m    431\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lemma_summary = news_raw['lemma_summary']\n",
    "for sum_num, summary in enumerate(lemma_summary):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['news_keyword_5'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))\n",
    "    news_raw['news_keyword_10'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10)\n",
    "#news_raw.to_csv('C:/projectnasdaq/project_test/news_raw.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목 정보 수집 ( yfinance 패키지 사용 ) => 수집완료"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 종목코드 리스트\n",
    "stocks = nasdaq_stock['symbol']\n",
    "# 종목코드에 대한 정보 수집용 데이터프레임 생성\n",
    "stock_info = pd.DataFrame(columns=['symbol', 'info'])\n",
    "i = 0\n",
    "\n",
    "for stock_num, stock in enumerate(stocks):\n",
    "    tickers = yf.Ticker(stock)\n",
    "    ticker = tickers.info\n",
    "    i = i + 1\n",
    "    #print(stock_num,stock, \"===>>\",ticker)\n",
    "    stock_info.loc[i] = [stock, ticker]\n",
    "    time.sleep(random.uniform(3, 4))\n",
    "\n",
    "#stock_info.to_csv('C:/projectnasdaq/project2/stock_info.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yahoo_dataset 파일 수정 ( yahoo_dataset_mapping.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 파일 수정 ( -> symbol name 수정 )\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset.csv')\n",
    "# yahoo_dataset에서 필요없는 컬럼들 지우고 csv 파일에 있는 종목코드에 종목명 데이터들 붙임\n",
    "\n",
    "yahoo_dataset.insert(1,'new_symbol','0')\n",
    "yahoo_dataset.insert(2,'name','0')\n",
    "yahoo_dataset.insert(3,'company_word','0')\n",
    "\n",
    "yahoo_dataset_symbol = yahoo_dataset['symbol']\n",
    "rep_stock_symbol = rep_stock['symbol']\n",
    "\n",
    "for i, j in enumerate(yahoo_dataset_symbol):\n",
    "    for k, l in enumerate(rep_stock_symbol):\n",
    "        if j == l:\n",
    "            yahoo_dataset['new_symbol'][i] = rep_stock['symbol'][k]\n",
    "            yahoo_dataset['name'][i] = rep_stock['name'][k]\n",
    "            yahoo_dataset['company_word'][i] = rep_stock['name_a'][k]\n",
    "            break\n",
    "\n",
    "# yahoo_dataset 컬럼 정리\n",
    "yahoo_dataset.drop(columns=[\"quoteType\", \"currency\", 'regularMarketPrice', 'regularMarketChange', 'regularMarketChangePercent',\n",
    "             'regularMarketVolume', 'averageDailyVolume3Month', 'marketCap', 'trailingPE', 'fiftyTwoWeekLow',\n",
    "             'fiftyTwoWeekHigh', 'regularMarketOpen', 'priceHint', 'underlyingSymbol'], inplace=True)\n",
    "#yahoo_dataset.to_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추출된 회사 이름이랑 ( 전처리 한 ) 뉴스 키워드 연결 ( company_word.csv 파일에 저장 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/company_word.csv')\n",
    "news_raw_keyword = pd.read_csv('C:/projectnasdaq/project_test/news_raw.csv') # 실행을 전부 돌리면 시간 오래 걸려서 미리 키워드 뽑아놓고 계속 파일 가져와서 썼기때문에 news_raw_keyword라는 이름으로 가져와서 씀 , news_raw 이름으로 그대로 가져오면 키워드 컬럼이 비워져있는 상태라서 오류 발생\n",
    "\n",
    "# 키워드 전처리\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].str.split(\" \")\n",
    "\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].str.split(\" \")\n",
    "\n",
    "company_word['stock_keyword_5'] = 0\n",
    "\n",
    "keyword_list = news_raw_keyword['news_keyword_5']\n",
    "\n",
    "news_raw_id = news_raw_keyword['news_id']\n",
    "company_word_id = company_word['news_id']\n",
    "\n",
    "for i, j in enumerate(company_word_id):\n",
    "    for k, l in enumerate(news_raw_id):\n",
    "        if j == l:\n",
    "            company_word['stock_keyword_5'][i] = news_raw_keyword['news_keyword_5'][k]\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목명이 추출된 뉴스 키워드들끼리 모으기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# company단어들 하나씩만 list에 담기 -> 단어 이름이 같은 종목명들의 키워드 모으기 위해서\n",
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/company_word.csv')\n",
    "#company_word.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "company_words = company_word['company_word']\n",
    "\n",
    "company_word_same_list = []\n",
    "for company in company_words:\n",
    "    if company not in company_word_same_list:\n",
    "        company_word_same_list.append(company)\n",
    "\n",
    "company_word_sames = pd.DataFrame(company_word_same_list, columns=['company_word_same'])\n",
    "company_word_same = company_word_sames['company_word_same']\n",
    "\n",
    "# 키워드랑 company_name이랑 데이터프레임에서 맵핑 시키기위한 새로운 dataframe 생성\n",
    "stock_info = pd.DataFrame(columns=['company', 'stock_keyword'])\n",
    "\n",
    "# company_name에 대한 키워드들 딕셔너리 형태로 모으기\n",
    "for i, j in enumerate(company_word_same):\n",
    "    stock_info.loc[i] = [j, company_word[company_word['company_word'] == j]['stock_keyword_5'].tolist()]\n",
    "\n",
    "# company_name_keyword_organize.csv에 column 삽입\n",
    "stock_info.insert(0, 'symbol', '0')\n",
    "stock_info.insert(1, 'name', '0')\n",
    "\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "company_word_ = company_word['company_word']\n",
    "company = stock_info['company']\n",
    "\n",
    "for i,j in enumerate(company_word_):\n",
    "    for k, l in enumerate(company):\n",
    "        if j==l:\n",
    "            stock_info['symbol'][k] = company_word['symbol'][i]\n",
    "            stock_info['name'][k] = company_word['name'][i]\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 키워드 리스트를 하나의 리스트로 합치기\n",
    "stock_keyword = stock_info['stock_keyword']\n",
    "for i, j in enumerate(stock_keyword):\n",
    "    stock_info['stock_keyword'][i] = ''.join(j)\n",
    "\n",
    "# 합친 키워드 리스트 전처리\n",
    "stock_info['stock_keyword'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'].replace(',',' ',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].str.split(\" \")\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# 키워드들 높은 빈도순으로 키워드 추출하기\n",
    "stock_info[['stock_frequency_keyword_4','stock_frequency_keyword_5']] = 0\n",
    "frequency_word = stock_info['stock_keyword']\n",
    "\n",
    "for i, j in enumerate(frequency_word):\n",
    "    # 대표 키워드 단어 빈도 순으로 4개 추출\n",
    "    stock_info['stock_frequency_keyword_4'][i] = Counter(j).most_common(4)\n",
    "#    # 대표 키워드 단어 빈도 순으로 5개 추출\n",
    "    stock_info['stock_frequency_keyword_5'][i] = Counter(j).most_common(5)\n",
    "\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) 뉴스 원문에 종목명의 대표 키워드가 포함될 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문에 대표 키워드가 포함될 경우 508\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑(lemmatize)\n",
    "lemma_tokenize_list = news_raw['lemma_tokenize']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "mapping_result = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(lemma_tokenize_list):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개가 교집합으로 있을 경우\n",
    "        if len(together)==5:\n",
    "            mapping_result.loc[a] = [news_raw['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k]]\n",
    "\n",
    "print('뉴스 원문에 대표 키워드가 포함될 경우',len(mapping_result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 : 974\n"
     ]
    }
   ],
   "source": [
    "mapping_result_2 = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5','news_keyword_10'])\n",
    "news_keyword_10 = news_raw_keyword['news_keyword_10']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_keyword_10):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            mapping_result_2.loc[a] = [news_raw_keyword['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw_keyword['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k],news_raw_keyword['news_keyword_10'][i]]\n",
    "\n",
    "print('뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 :',len(mapping_result_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "mapping_result = mapping_result.drop(columns=['stock_frequency_keyword_5'])\n",
    "mapping_result_2 = mapping_result_2.drop(columns=['stock_frequency_keyword_5', 'news_keyword_10'])\n",
    "mapping_result_final = pd.concat([mapping_result,mapping_result_2], ignore_index=True)\n",
    "overlap = mapping_result_final.drop_duplicates(['news_id','symbol'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "     news_id symbol                              name  \\\n0     345585    DEO                        Diageo plc   \n1     345591   ATSG  Air Transport Services Group Inc   \n2     345620   COST      Costco Wholesale Corporation   \n3     345620     DG        Dollar General Corporation   \n4     345620   BBWI             Bath & Body Works Inc   \n...      ...    ...                               ...   \n1477  443714    LEN                Lennar Corporation   \n1478  443716   GDRX               GoodRx Holdings Inc   \n1479  443719    SPI                     SPI Energy Co   \n1480  443747    PBR             Petroleo Brasileiro S   \n1481  443747    BAK                    Braskem SA ADR   \n\n                                          lemma_summary  \n0     nestle sa sailed past ailing consumer good riv...  \n1     nadine scheiner s effort to travel from her ho...  \n2     walmart inc might be one of the few big winner...  \n3     walmart inc might be one of the few big winner...  \n4     walmart inc might be one of the few big winner...  \n...                                                 ...  \n1477  mizuho financial group inc plan to trim office...  \n1478  a sharp recovery in health care dealmaking wil...  \n1479  in a panel discussion during the travel indust...  \n1480  just one week after the u s federal reserve se...  \n1481  just one week after the u s federal reserve se...  \n\n[1265 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>lemma_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>345585</td>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>345591</td>\n      <td>ATSG</td>\n      <td>Air Transport Services Group Inc</td>\n      <td>nadine scheiner s effort to travel from her ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345620</td>\n      <td>COST</td>\n      <td>Costco Wholesale Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>345620</td>\n      <td>DG</td>\n      <td>Dollar General Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>345620</td>\n      <td>BBWI</td>\n      <td>Bath &amp; Body Works Inc</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>443714</td>\n      <td>LEN</td>\n      <td>Lennar Corporation</td>\n      <td>mizuho financial group inc plan to trim office...</td>\n    </tr>\n    <tr>\n      <th>1478</th>\n      <td>443716</td>\n      <td>GDRX</td>\n      <td>GoodRx Holdings Inc</td>\n      <td>a sharp recovery in health care dealmaking wil...</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>443719</td>\n      <td>SPI</td>\n      <td>SPI Energy Co</td>\n      <td>in a panel discussion during the travel indust...</td>\n    </tr>\n    <tr>\n      <th>1480</th>\n      <td>443747</td>\n      <td>PBR</td>\n      <td>Petroleo Brasileiro S</td>\n      <td>just one week after the u s federal reserve se...</td>\n    </tr>\n    <tr>\n      <th>1481</th>\n      <td>443747</td>\n      <td>BAK</td>\n      <td>Braskem SA ADR</td>\n      <td>just one week after the u s federal reserve se...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1265 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" 화요일에 와서 해야할 일 : shortname가 business summary에 포함되면 short name 지우고, 소문자화 한 후에\n",
    "     business summary 단복수 처리 후 키워드 다섯개 뽑아내기, 그 후에 키워드 전처리, 뉴스 원문에 키워드 포함시 맵핑 &\n",
    "      뉴스원문 키워드와 교집합이 있을 경우 맵핑, 전부 끝나면 edgar 제대로 된 데이터 수집하기\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "yahoo_dataset['longBusinessSummary'] = 0\n",
    "yahoo_dataset['company_info'] = yahoo_dataset['company_info'].fillna(\"0\")\n",
    "company_info = yahoo_dataset['company_info']\n",
    "\n",
    "for i,j in enumerate(company_info):\n",
    "    keys =[]\n",
    "    values = []\n",
    "    company_info_list = j.split(', \"')\n",
    "    for info_list in company_info_list:\n",
    "        pair = info_list.split('\":')\n",
    "        try:\n",
    "            keys.append(pair[0])\n",
    "            values.append(pair[1])\n",
    "            my_dict = dict(zip(keys, values))\n",
    "            BusinessSummary= my_dict.get('longBusinessSummary')\n",
    "            yahoo_dataset['longBusinessSummary'][i] = BusinessSummary\n",
    "        except IndexError: continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].str.lower() #소문자화\n",
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].str.lower() #소문자화\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "long_business_summary = yahoo_dataset['longBusinessSummary']\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].fillna(\"NaN_data\")\n",
    "shortname = yahoo_dataset['shortName']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 종목명 설명 컬럼에 종목명이 포함되면 지우기\n",
    "for i,j in enumerate(long_business_summary):\n",
    "    for k,l in enumerate(shortname):\n",
    "        if l in j:\n",
    "            yahoo_dataset['longBusinessSummary'][i] = yahoo_dataset['longBusinessSummary'][k].replace(l,'')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "long_business_summary_stopwords = {'does','not','have','company','corp','inc'}\n",
    "yahoo_dataset['longBusinessSummary'].replace(long_business_summary_stopwords, '', regex=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "long_business_summary_token = yahoo_dataset['longBusinessSummary'].str.split(\" \")\n",
    "long_business_summary_token = long_business_summary_token.apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "for i,j in enumerate(long_business_summary_token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    yahoo_dataset['longBusinessSummary'][i] = lemma_word\n",
    "    yahoo_dataset['longBusinessSummary'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0 symbol new_symbol                          name  \\\n0              0      A          A      Agilent Technologies Inc   \n1              1     AA         AA             Alcoa Corporation   \n2              2    AAC        AAC  Ares Acquisition Corporation   \n3              3   AACG       AACG         ATA Creativity Global   \n4              4   AACI       AACI       Armada Acquisition Corp   \n...          ...    ...        ...                           ...   \n9001        9001   ZYME       ZYME                 Zymeworks Inc   \n9002        9002   ZYNE       ZYNE   Zynerba Pharmaceuticals Inc   \n9003        9003   ZYXI       ZYXI                     Zynex Inc   \n9004        9004   TRUE       TRUE                   TrueCar Inc   \n9005        9005    NaN          0                             0   \n\n                 company_word                     shortName  \\\n0        agilent,technologies     agilent technologies  inc   \n1           alcoa,corporation             alcoa corporation   \n2            ares,acquisition  ares acquisition corporation   \n3              ata,creativity         ata creativity global   \n4          armada,acquisition    armada acquisition corp  i   \n...                       ...                           ...   \n9001                zymeworks                 zymeworks inc   \n9002  zynerba,pharmaceuticals  zynerba pharmaceuticals  inc   \n9003                    zynex                    zynex  inc   \n9004                  truecar                  truecar  inc   \n9005                        0                 nano labs ltd   \n\n                           longName  \\\n0        Agilent Technologies, Inc.   \n1                 Alcoa Corporation   \n2      Ares Acquisition Corporation   \n3             ATA Creativity Global   \n4        Armada Acquisition Corp. I   \n...                             ...   \n9001                 Zymeworks Inc.   \n9002  Zynerba Pharmaceuticals, Inc.   \n9003                    Zynex, Inc.   \n9004                  TrueCar, Inc.   \n9005                  Nano Labs Ltd   \n\n                                           company_info  \\\n0     {\"address1\": \"5301 Stevens Creek Boulevard\", \"...   \n1     {\"address1\": \"201 Isabella Street\", \"address2\"...   \n2     {\"address1\": \"245 Park Avenue\", \"address2\": \"4...   \n3     {\"address1\": \"Building No. 2\", \"address2\": \"Ea...   \n4     {\"address1\": \"2005 Market Street\", \"address2\":...   \n...                                                 ...   \n9001  {\"address1\": \"114 East 4th Avenue\", \"address2\"...   \n9002  {\"address1\": \"80 West Lancaster Avenue\", \"addr...   \n9003  {\"address1\": \"9655 Maroon Circle\", \"city\": \"En...   \n9004  {\"address1\": \"120 Broadway\", \"address2\": \"Suit...   \n9005  {\"address1\": \"Dikaiyinzuo\", \"address2\": \"30th ...   \n\n                                           finance_info  \\\n0     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n1     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n2     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n3     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n4     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n...                                                 ...   \n9001  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9002  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9003  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9004  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9005  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n\n                                    longBusinessSummary  \n0     provides application focused solution to the l...  \n1     together with it subsidiary produce and sell b...  \n2     significant operation it intends to effect a m...  \n3     together with it subsidiary provides education...  \n4     focus on effecting a merger capital stock exch...  \n...                                                 ...  \n9001  discovers develops manufacture and delivers hu...  \n9002  operates a a clinical stage specialty pharmace...  \n9003  through it subsidiary design manufacture and m...  \n9004  significant operation the intends to effect a ...  \n9005  significant operation the intends to effect a ...  \n\n[9006 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>symbol</th>\n      <th>new_symbol</th>\n      <th>name</th>\n      <th>company_word</th>\n      <th>shortName</th>\n      <th>longName</th>\n      <th>company_info</th>\n      <th>finance_info</th>\n      <th>longBusinessSummary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A</td>\n      <td>A</td>\n      <td>Agilent Technologies Inc</td>\n      <td>agilent,technologies</td>\n      <td>agilent technologies  inc</td>\n      <td>Agilent Technologies, Inc.</td>\n      <td>{\"address1\": \"5301 Stevens Creek Boulevard\", \"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>provides application focused solution to the l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>AA</td>\n      <td>AA</td>\n      <td>Alcoa Corporation</td>\n      <td>alcoa,corporation</td>\n      <td>alcoa corporation</td>\n      <td>Alcoa Corporation</td>\n      <td>{\"address1\": \"201 Isabella Street\", \"address2\"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>together with it subsidiary produce and sell b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>Ares Acquisition Corporation</td>\n      <td>ares,acquisition</td>\n      <td>ares acquisition corporation</td>\n      <td>Ares Acquisition Corporation</td>\n      <td>{\"address1\": \"245 Park Avenue\", \"address2\": \"4...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation it intends to effect a m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>AACG</td>\n      <td>AACG</td>\n      <td>ATA Creativity Global</td>\n      <td>ata,creativity</td>\n      <td>ata creativity global</td>\n      <td>ATA Creativity Global</td>\n      <td>{\"address1\": \"Building No. 2\", \"address2\": \"Ea...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>together with it subsidiary provides education...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>AACI</td>\n      <td>AACI</td>\n      <td>Armada Acquisition Corp</td>\n      <td>armada,acquisition</td>\n      <td>armada acquisition corp  i</td>\n      <td>Armada Acquisition Corp. I</td>\n      <td>{\"address1\": \"2005 Market Street\", \"address2\":...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>focus on effecting a merger capital stock exch...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9001</th>\n      <td>9001</td>\n      <td>ZYME</td>\n      <td>ZYME</td>\n      <td>Zymeworks Inc</td>\n      <td>zymeworks</td>\n      <td>zymeworks inc</td>\n      <td>Zymeworks Inc.</td>\n      <td>{\"address1\": \"114 East 4th Avenue\", \"address2\"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>discovers develops manufacture and delivers hu...</td>\n    </tr>\n    <tr>\n      <th>9002</th>\n      <td>9002</td>\n      <td>ZYNE</td>\n      <td>ZYNE</td>\n      <td>Zynerba Pharmaceuticals Inc</td>\n      <td>zynerba,pharmaceuticals</td>\n      <td>zynerba pharmaceuticals  inc</td>\n      <td>Zynerba Pharmaceuticals, Inc.</td>\n      <td>{\"address1\": \"80 West Lancaster Avenue\", \"addr...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>operates a a clinical stage specialty pharmace...</td>\n    </tr>\n    <tr>\n      <th>9003</th>\n      <td>9003</td>\n      <td>ZYXI</td>\n      <td>ZYXI</td>\n      <td>Zynex Inc</td>\n      <td>zynex</td>\n      <td>zynex  inc</td>\n      <td>Zynex, Inc.</td>\n      <td>{\"address1\": \"9655 Maroon Circle\", \"city\": \"En...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>through it subsidiary design manufacture and m...</td>\n    </tr>\n    <tr>\n      <th>9004</th>\n      <td>9004</td>\n      <td>TRUE</td>\n      <td>TRUE</td>\n      <td>TrueCar Inc</td>\n      <td>truecar</td>\n      <td>truecar  inc</td>\n      <td>TrueCar, Inc.</td>\n      <td>{\"address1\": \"120 Broadway\", \"address2\": \"Suit...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation the intends to effect a ...</td>\n    </tr>\n    <tr>\n      <th>9005</th>\n      <td>9005</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nano labs ltd</td>\n      <td>Nano Labs Ltd</td>\n      <td>{\"address1\": \"Dikaiyinzuo\", \"address2\": \"30th ...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation the intends to effect a ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9006 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yahoo_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}