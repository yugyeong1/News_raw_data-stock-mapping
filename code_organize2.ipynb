{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "news_raw = pd.read_csv(\"C:/projectnasdaq/news_raw.csv\", encoding='latin1') # 뉴스 raw data\n",
    "nasdaq_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks.csv\", encoding='latin1') # stock_list data\n",
    "rep_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks_refine_total.csv\", encoding='latin1') # 대표단어 csv\n",
    "\n",
    "# 테스트 용도로 데이터 1000개\n",
    "news_raw = news_raw.head(1000)\n",
    "news_raw = news_raw[['news_id','title','summary']]\n",
    "nasdaq_stock = nasdaq_stock[['pk','symbol','name']]\n",
    "rep_stock = rep_stock[['pk','symbol','name','name_a']]\n",
    "\n",
    "# new_raw 데이터 전처리\n",
    "news_raw['summary'] = news_raw['summary'].str.lower() #소문자화\n",
    "news_raw['summary'] = news_raw['summary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "news_raw['summary'].replace('bloomberg', '', regex=True, inplace=True)\n",
    "\n",
    "# news_raw 데이터 토큰화\n",
    "news_raw['tokenize'] = 0\n",
    "news_raw['tokenize'] = news_raw['summary'].str.split(\" \")\n",
    "news_raw['tokenize'] = news_raw['tokenize'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# news raw 데이터에서 필요한 컬럼 생성\n",
    "news_raw[['lemma_summary','lemma_tokenize','news_keyword_5','news_keyword_10']] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 회사 이름 추출 => 추출 완료 ( company_word.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8072\\4084188830.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     38\u001B[0m                                                      rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n\u001B[0;32m     39\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m                     \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[0mcompany_word\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcompany_word\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnasdaq_stock\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'left'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mleft_on\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pk'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright_on\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pk'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 회사이름 추출 데이터 저장용도\n",
    "# list에 담아주기\n",
    "tokenize_list = news_raw['tokenize']\n",
    "rep_list = rep_stock['name_a'].str.split(\",\")\n",
    "\n",
    "company_word = pd.DataFrame(columns=['news_id', 'company_word','pk'])\n",
    "\n",
    "# 대표단어 csv파일에서 대표단어가 포함되면 맵핑\n",
    "i = 0\n",
    "\n",
    "for token_num, token in enumerate(tokenize_list):  # news_raw data 토큰화\n",
    "    for rep_num, rep in enumerate(rep_list):  # 대표단어 csv파일에서 대표단어에 해당\n",
    "        i = i + 1\n",
    "        if len(rep) == 1:\n",
    "            if rep[0] in token:\n",
    "                company_word.loc[i] = [news_raw['news_id'][token_num], rep[0], rep_stock['pk'][rep_num]]\n",
    "\n",
    "        elif len(rep) == 2:  # 대표단어가 2개로 된 단어일 때\n",
    "            if rep[0] in token:  # 대표단어의 첫번째 단어가 org단어에 있으면\n",
    "                found = token.index(rep[0])  # 대표단어의 첫번째 단어와 일치하는 org단어의 인덱스 위치 번호\n",
    "                try:\n",
    "                    search = found + 1  # stocklist의 첫번째 단어가 org에 포함됐을때 그 다음 단어\n",
    "                    search_found = token[search]  # org의 (+1을 한) 다음 단어에 해당\n",
    "                    if rep[1] == search_found:\n",
    "                        company_word.loc[i] = [news_raw['news_id'][token_num], rep[0] + \" \" + rep[1],\n",
    "                                                 rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        elif len(rep) == 3:\n",
    "            if rep[0] in token:\n",
    "                try:\n",
    "                    search_found = token[token.index(rep[0]) + 1]\n",
    "                    if rep[1] == search_found:\n",
    "                        two_found = token[token.index(rep[0]) + 2]\n",
    "                        if rep[2] == two_found:\n",
    "                            company_word.loc[i] = [news_raw['news_id'][token_num],\n",
    "                                                     rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "company_word = pd.merge(company_word, nasdaq_stock, how='left', left_on='pk', right_on='pk')\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 원문에서 키워드 추출 ( Keybert 사용 )=> 추출 완료 ( news_raw.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer 패키지로 단복수 문제 처리\n",
    "token = news_raw['tokenize']\n",
    "\n",
    "for i, j in enumerate(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    news_raw['lemma_tokenize'][i] = lemma_word\n",
    "    news_raw['lemma_summary'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8072\\639550318.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'news_keyword_5'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'news_keyword_10'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtop_n\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;31m#news_raw.to_csv('C:/projectnasdaq/project_test/news_raw.csv')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\_model.py\u001B[0m in \u001B[0;36mextract_keywords\u001B[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001B[0m\n\u001B[0;32m    153\u001B[0m         \u001B[1;31m# Extract embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[0mdoc_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m         \u001B[0mword_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[1;31m# Find keywords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\u001B[0m in \u001B[0;36membed\u001B[1;34m(self, documents, verbose)\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mthat\u001B[0m \u001B[0meach\u001B[0m \u001B[0mhave\u001B[0m \u001B[0man\u001B[0m \u001B[0membeddings\u001B[0m \u001B[0msize\u001B[0m \u001B[0mof\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mm\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m         \"\"\"\n\u001B[1;32m---> 62\u001B[1;33m         \u001B[0membeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocuments\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     63\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0membeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36mencode\u001B[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[0;32m    159\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mstart_index\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Batches\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mnot\u001B[0m \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m             \u001B[0msentences_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msentences_sorted\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstart_index\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstart_index\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 161\u001B[1;33m             \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    162\u001B[0m             \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_to_device\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    317\u001B[0m         \u001B[0mTokenizes\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m         \"\"\"\n\u001B[1;32m--> 319\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_first_module\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_sentence_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m    111\u001B[0m             \u001B[0mto_tokenize\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcol\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mto_tokenize\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m         \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mto_tokenize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtruncation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'longest_first'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"pt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_seq_length\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2532\u001B[0m                 \u001B[0mreturn_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2533\u001B[0m                 \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2534\u001B[1;33m                 \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2535\u001B[0m             )\n\u001B[0;32m   2536\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mbatch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2723\u001B[0m             \u001B[0mreturn_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2724\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2725\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2726\u001B[0m         )\n\u001B[0;32m   2727\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001B[0m in \u001B[0;36m_batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[0;32m    427\u001B[0m             \u001B[0mbatch_text_or_text_pairs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m             \u001B[0madd_special_tokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0madd_special_tokens\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 429\u001B[1;33m             \u001B[0mis_pretokenized\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mis_split_into_words\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    430\u001B[0m         )\n\u001B[0;32m    431\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lemma_summary = news_raw['lemma_summary']\n",
    "for sum_num, summary in enumerate(lemma_summary):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['news_keyword_5'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))\n",
    "    news_raw['news_keyword_10'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10)\n",
    "#news_raw.to_csv('C:/projectnasdaq/project_test/news_raw.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목 정보 수집 ( yfinance 패키지 사용 ) => 수집완료"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 종목코드 리스트\n",
    "stocks = nasdaq_stock['symbol']\n",
    "# 종목코드에 대한 정보 수집용 데이터프레임 생성\n",
    "stock_info = pd.DataFrame(columns=['symbol', 'info'])\n",
    "i = 0\n",
    "\n",
    "for stock_num, stock in enumerate(stocks):\n",
    "    tickers = yf.Ticker(stock)\n",
    "    ticker = tickers.info\n",
    "    i = i + 1\n",
    "    #print(stock_num,stock, \"===>>\",ticker)\n",
    "    stock_info.loc[i] = [stock, ticker]\n",
    "    time.sleep(random.uniform(3, 4))\n",
    "\n",
    "#stock_info.to_csv('C:/projectnasdaq/project2/stock_info.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yahoo_dataset 파일 수정 ( yahoo_dataset_mapping.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 파일 수정 ( -> symbol name 수정 )\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset.csv')\n",
    "\n",
    "# yahoo_dataset에서 필요없는 컬럼들 지우고 csv 파일에 있는 종목코드에 종목명 데이터들 붙임\n",
    "yahoo_dataset.insert(1,'new_symbol','0')\n",
    "yahoo_dataset.insert(2,'name','0')\n",
    "yahoo_dataset.insert(3,'company_word','0')\n",
    "\n",
    "yahoo_dataset_symbol = yahoo_dataset['symbol']\n",
    "rep_stock_symbol = rep_stock['symbol']\n",
    "\n",
    "for i, j in enumerate(yahoo_dataset_symbol):\n",
    "    for k, l in enumerate(rep_stock_symbol):\n",
    "        if j == l:\n",
    "            yahoo_dataset['new_symbol'][i] = rep_stock['symbol'][k]\n",
    "            yahoo_dataset['name'][i] = rep_stock['name'][k]\n",
    "            yahoo_dataset['company_word'][i] = rep_stock['name_a'][k]\n",
    "            break\n",
    "\n",
    "# yahoo_dataset 컬럼 정리\n",
    "yahoo_dataset.drop(columns=[\"quoteType\", \"currency\", 'regularMarketPrice', 'regularMarketChange', 'regularMarketChangePercent',\n",
    "             'regularMarketVolume', 'averageDailyVolume3Month', 'marketCap', 'trailingPE', 'fiftyTwoWeekLow',\n",
    "             'fiftyTwoWeekHigh', 'regularMarketOpen', 'priceHint', 'underlyingSymbol'], inplace=True)\n",
    "#yahoo_dataset.to_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추출된 회사 이름이랑 ( 전처리 한 ) 뉴스 키워드 연결 ( company_word.csv 파일에 저장 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/company_word.csv')\n",
    "news_raw_keyword = pd.read_csv('C:/projectnasdaq/project_test/news_raw.csv') # 실행을 전부 돌리면 시간 오래 걸려서 미리 키워드 뽑아놓고 계속 파일 가져와서 썼기때문에 news_raw_keyword라는 이름으로 가져와서 씀 , news_raw 이름으로 그대로 가져오면 키워드 컬럼이 비워져있는 상태라서 오류 발생\n",
    "\n",
    "# 키워드 전처리\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_5'] = news_raw_keyword['news_keyword_5'].str.split(\" \")\n",
    "\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "news_raw_keyword['news_keyword_10'] = news_raw_keyword['news_keyword_10'].str.split(\" \")\n",
    "\n",
    "company_word['stock_keyword_5'] = 0\n",
    "\n",
    "keyword_list = news_raw_keyword['news_keyword_5']\n",
    "\n",
    "news_raw_id = news_raw_keyword['news_id']\n",
    "company_word_id = company_word['news_id']\n",
    "\n",
    "for i, j in enumerate(company_word_id):\n",
    "    for k, l in enumerate(news_raw_id):\n",
    "        if j == l:\n",
    "            company_word['stock_keyword_5'][i] = news_raw_keyword['news_keyword_5'][k]\n",
    "#company_word.to_csv('C:/projectnasdaq/project_test/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목명이 추출된 뉴스 키워드들끼리 모으기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# company단어들 하나씩만 list에 담기 -> 단어 이름이 같은 종목명들의 키워드 모으기 위해서\n",
    "company_word = pd.read_csv('C:/projectnasdaq/project_test/company_word.csv')\n",
    "#company_word.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "company_words = company_word['company_word']\n",
    "\n",
    "company_word_same_list = []\n",
    "for company in company_words:\n",
    "    if company not in company_word_same_list:\n",
    "        company_word_same_list.append(company)\n",
    "\n",
    "company_word_sames = pd.DataFrame(company_word_same_list, columns=['company_word_same'])\n",
    "company_word_same = company_word_sames['company_word_same']\n",
    "\n",
    "# 키워드랑 company_name이랑 데이터프레임에서 맵핑 시키기위한 새로운 dataframe 생성\n",
    "stock_info = pd.DataFrame(columns=['company', 'stock_keyword'])\n",
    "\n",
    "# company_name에 대한 키워드들 딕셔너리 형태로 모으기\n",
    "for i, j in enumerate(company_word_same):\n",
    "    stock_info.loc[i] = [j, company_word[company_word['company_word'] == j]['stock_keyword_5'].tolist()]\n",
    "\n",
    "# company_name_keyword_organize.csv에 column 삽입\n",
    "stock_info.insert(0, 'symbol', '0')\n",
    "stock_info.insert(1, 'name', '0')\n",
    "\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "company_word_ = company_word['company_word']\n",
    "company = stock_info['company']\n",
    "\n",
    "for i,j in enumerate(company_word_):\n",
    "    for k, l in enumerate(company):\n",
    "        if j==l:\n",
    "            stock_info['symbol'][k] = company_word['symbol'][i]\n",
    "            stock_info['name'][k] = company_word['name'][i]\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 키워드 리스트를 하나의 리스트로 합치기\n",
    "stock_keyword = stock_info['stock_keyword']\n",
    "for i, j in enumerate(stock_keyword):\n",
    "    stock_info['stock_keyword'][i] = ''.join(j)\n",
    "\n",
    "# 합친 키워드 리스트 전처리\n",
    "stock_info['stock_keyword'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'].replace(',',' ',regex=True, inplace=True)\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].str.split(\" \")\n",
    "stock_info['stock_keyword'] = stock_info['stock_keyword'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# 키워드들 높은 빈도순으로 키워드 추출하기\n",
    "stock_info[['stock_frequency_keyword_4','stock_frequency_keyword_5']] = 0\n",
    "frequency_word = stock_info['stock_keyword']\n",
    "\n",
    "for i, j in enumerate(frequency_word):\n",
    "    # 대표 키워드 단어 빈도 순으로 4개 추출\n",
    "    stock_info['stock_frequency_keyword_4'][i] = Counter(j).most_common(4)\n",
    "#    # 대표 키워드 단어 빈도 순으로 5개 추출\n",
    "    stock_info['stock_frequency_keyword_5'][i] = Counter(j).most_common(5)\n",
    "\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_4'] = stock_info['stock_frequency_keyword_4'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].str.split(\" \")\n",
    "stock_info['stock_frequency_keyword_5'] = stock_info['stock_frequency_keyword_5'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) 뉴스 원문에 종목명의 대표 키워드가 포함될 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문에 대표 키워드가 포함될 경우 508\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑(lemmatize)\n",
    "lemma_tokenize_list = news_raw['lemma_tokenize']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "mapping_result = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(lemma_tokenize_list):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개가 교집합으로 있을 경우\n",
    "        if len(together)==5:\n",
    "            mapping_result.loc[a] = [news_raw['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k]]\n",
    "\n",
    "print('뉴스 원문에 대표 키워드가 포함될 경우',len(mapping_result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 : 974\n"
     ]
    }
   ],
   "source": [
    "mapping_result_2 = pd.DataFrame(columns=['news_id', 'symbol','name','lemma_summary','stock_frequency_keyword_5','news_keyword_10'])\n",
    "news_keyword_10 = news_raw_keyword['news_keyword_10']\n",
    "stock_frequency_keyword_5 = stock_info['stock_frequency_keyword_5']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_keyword_10):\n",
    "    for k,l in enumerate(stock_frequency_keyword_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            mapping_result_2.loc[a] = [news_raw_keyword['news_id'][i], stock_info['symbol'][k], stock_info['name'][k],news_raw_keyword['lemma_summary'][i], stock_info['stock_frequency_keyword_5'][k],news_raw_keyword['news_keyword_10'][i]]\n",
    "\n",
    "print('뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 :',len(mapping_result_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 위 두가지 방법을 합쳤을 때 중복 행 제거\n",
    "mapping_result = mapping_result.drop(columns=['stock_frequency_keyword_5'])\n",
    "mapping_result_2 = mapping_result_2.drop(columns=['stock_frequency_keyword_5', 'news_keyword_10'])\n",
    "mapping_result_final = pd.concat([mapping_result,mapping_result_2], ignore_index=True)\n",
    "overlap = mapping_result_final.drop_duplicates(['news_id','symbol'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "overlap.to_csv('C:/projectnasdaq/project_test/overlap.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" 화요일에 와서 해야할 일 : shortname가 business summary에 포함되면 short name 지우고, 소문자화 한 후에\n",
    "     business summary 단복수 처리 후 키워드 다섯개 뽑아내기 ==> 진행 중\n",
    "      그 후에 키워드 전처리, 뉴스 원문에 키워드 포함시 맵핑 &\n",
    "      뉴스원문 키워드와 교집합이 있을 경우 맵핑, 전부 끝나면 edgar 제대로 된 데이터 수집하기\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project_test/yahoo_dataset.csv')\n",
    "yahoo_dataset[['longBusinessSummary','longBusinessSummary_keyword']] = 0\n",
    "yahoo_dataset['company_info'] = yahoo_dataset['company_info'].fillna(\"0\")\n",
    "company_info = yahoo_dataset['company_info']\n",
    "\n",
    "# yfinance company_info에서 longBusinessSummary 부분 추출\n",
    "for i,j in enumerate(company_info):\n",
    "    keys =[]\n",
    "    values = []\n",
    "    company_info_list = j.split(', \"')\n",
    "    for info_list in company_info_list:\n",
    "        pair = info_list.split('\":')\n",
    "        try:\n",
    "            keys.append(pair[0])\n",
    "            values.append(pair[1])\n",
    "            my_dict = dict(zip(keys, values))\n",
    "            BusinessSummary= my_dict.get('longBusinessSummary')\n",
    "            yahoo_dataset['longBusinessSummary'][i] = BusinessSummary\n",
    "        except IndexError: continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# longBusinessSummary column 소문자화\n",
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].str.lower() #소문자화\n",
    "yahoo_dataset['longBusinessSummary'] = yahoo_dataset['longBusinessSummary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "# shortName column 소문자화\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].str.lower() #소문자화\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "long_business_summary = yahoo_dataset['longBusinessSummary']\n",
    "yahoo_dataset['shortName'] = yahoo_dataset['shortName'].fillna(\"\")\n",
    "shortname = yahoo_dataset['shortName']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 종목명 설명 컬럼에 종목명이 포함되면 지우기\n",
    "for i,j in enumerate(long_business_summary):\n",
    "    for k,l in enumerate(shortname):\n",
    "        if l in j:\n",
    "            yahoo_dataset['longBusinessSummary'][i] = yahoo_dataset['longBusinessSummary'][k].replace(l,'')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "long_business_summary_stopwords = {'does','not','have','company','corp','inc'}\n",
    "yahoo_dataset['longBusinessSummary'].replace(long_business_summary_stopwords, '', regex=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# longBusinessSummary 단복수 처리\n",
    "long_business_summary_token = yahoo_dataset['longBusinessSummary'].str.split(\" \")\n",
    "long_business_summary_token = long_business_summary_token.apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "for i,j in enumerate(long_business_summary_token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    yahoo_dataset['longBusinessSummary'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3568\\966386875.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mlong_business_summary\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0myahoo_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'longBusinessSummary'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msum_num\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummary\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlong_business_summary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0myahoo_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'longBusinessSummary_keyword'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\_model.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m     49\u001B[0m                       \u001B[1;33m*\u001B[0m \u001B[0mhttps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m//\u001B[0m\u001B[0mwww\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msbert\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mpretrained_models\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhtml\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \"\"\"\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect_backend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     def extract_keywords(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_utils.py\u001B[0m in \u001B[0;36mselect_backend\u001B[1;34m(embedding_model)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;31m# Create a Sentence Transformer model based on a string\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mSentenceTransformerBackend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;31m# Hugging Face embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, embedding_model)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0membedding_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSentenceTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m             raise ValueError(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'modules.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m#Load as SentenceTransformer model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_sbert_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m   \u001B[1;31m#Load with AutoModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_auto_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m_load_sbert_model\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule_config\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodules_config\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    839\u001B[0m             \u001B[0mmodule_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimport_from_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'type'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 840\u001B[1;33m             \u001B[0mmodule\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'path'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    841\u001B[0m             \u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    842\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(input_path)\u001B[0m\n\u001B[0;32m    135\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msbert_config_path\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfIn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m             \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfIn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtokenizer_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m_load_model\u001B[1;34m(self, model_name_or_path, config, cache_dir)\u001B[0m\n\u001B[0;32m     47\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m             \u001B[0mmodel_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_model_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mmodel_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m         raise ValueError(\n\u001B[0;32m    448\u001B[0m             \u001B[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2105\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mContextManagers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minit_contexts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2106\u001B[1;33m             \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2108\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdevice_map\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"auto\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config, add_pooling_layer)\u001B[0m\n\u001B[0;32m    885\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 887\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEmbeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    888\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEncoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    889\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mposition_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_position_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoken_type_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_vocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_weight\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfactory_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 140\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    141\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_weight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36mreset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m         \u001B[0minit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    150\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fill_padding_idx_with_zero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36mnormal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m    153\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhas_torch_function_variadic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtrunc_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m2.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2.\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36m_no_grad_normal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# longBusinessSummary 키워드 추출\n",
    "long_business_summary =  yahoo_dataset['longBusinessSummary']\n",
    "for sum_num, summary in enumerate(long_business_summary):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    yahoo_dataset['longBusinessSummary_keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yahoo_dataset.to_csv('C:/projectnasdaq/project_test/yahoo_dataset_keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# 뽑아낸 keyword 전처리\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].str.split(\" \")\n",
    "yahoo_dataset['longBusinessSummary_keyword'] = yahoo_dataset['longBusinessSummary_keyword'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0 symbol new_symbol                          name  \\\n0              0      A          A      Agilent Technologies Inc   \n1              1     AA         AA             Alcoa Corporation   \n2              2    AAC        AAC  Ares Acquisition Corporation   \n3              3   AACG       AACG         ATA Creativity Global   \n4              4   AACI       AACI       Armada Acquisition Corp   \n...          ...    ...        ...                           ...   \n9001        9001   ZYME       ZYME                 Zymeworks Inc   \n9002        9002   ZYNE       ZYNE   Zynerba Pharmaceuticals Inc   \n9003        9003   ZYXI       ZYXI                     Zynex Inc   \n9004        9004   TRUE       TRUE                   TrueCar Inc   \n9005        9005    NaN          0                             0   \n\n                 company_word                     shortName  \\\n0        agilent,technologies     agilent technologies  inc   \n1           alcoa,corporation             alcoa corporation   \n2            ares,acquisition  ares acquisition corporation   \n3              ata,creativity         ata creativity global   \n4          armada,acquisition    armada acquisition corp  i   \n...                       ...                           ...   \n9001                zymeworks                 zymeworks inc   \n9002  zynerba,pharmaceuticals  zynerba pharmaceuticals  inc   \n9003                    zynex                    zynex  inc   \n9004                  truecar                  truecar  inc   \n9005                        0                 nano labs ltd   \n\n                           longName  \\\n0        Agilent Technologies, Inc.   \n1                 Alcoa Corporation   \n2      Ares Acquisition Corporation   \n3             ATA Creativity Global   \n4        Armada Acquisition Corp. I   \n...                             ...   \n9001                 Zymeworks Inc.   \n9002  Zynerba Pharmaceuticals, Inc.   \n9003                    Zynex, Inc.   \n9004                  TrueCar, Inc.   \n9005                  Nano Labs Ltd   \n\n                                           company_info  \\\n0     {\"address1\": \"5301 Stevens Creek Boulevard\", \"...   \n1     {\"address1\": \"201 Isabella Street\", \"address2\"...   \n2     {\"address1\": \"245 Park Avenue\", \"address2\": \"4...   \n3     {\"address1\": \"Building No. 2\", \"address2\": \"Ea...   \n4     {\"address1\": \"2005 Market Street\", \"address2\":...   \n...                                                 ...   \n9001  {\"address1\": \"114 East 4th Avenue\", \"address2\"...   \n9002  {\"address1\": \"80 West Lancaster Avenue\", \"addr...   \n9003  {\"address1\": \"9655 Maroon Circle\", \"city\": \"En...   \n9004  {\"address1\": \"120 Broadway\", \"address2\": \"Suit...   \n9005  {\"address1\": \"Dikaiyinzuo\", \"address2\": \"30th ...   \n\n                                           finance_info  \\\n0     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n1     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n2     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n3     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n4     {'language': 'en-US', 'region': 'US', 'quoteTy...   \n...                                                 ...   \n9001  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9002  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9003  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9004  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n9005  {'language': 'en-US', 'region': 'US', 'quoteTy...   \n\n                                    longBusinessSummary  \\\n0     provides application focused solution to the l...   \n1     together with it subsidiary produce and sell b...   \n2     significant operation it intends to effect a m...   \n3     together with it subsidiary provides education...   \n4     focus on effecting a merger capital stock exch...   \n...                                                 ...   \n9001  discovers develops manufacture and delivers hu...   \n9002  operates a a clinical stage specialty pharmace...   \n9003  through it subsidiary design manufacture and m...   \n9004  significant operation the intends to effect a ...   \n9005  significant operation the intends to effect a ...   \n\n                            longBusinessSummary_keyword  \n0     [(chromatography, 0.4066), (spectrometry, 0.40...  \n1     [(alumina, 0.5381), (alcoa, 0.4941), (aluminum...  \n2     [(merger, 0.6149), (acquisition, 0.4213), (exc...  \n3     [(ata, 0.4455), (training, 0.3426), (education...  \n4     [(merger, 0.6285), (acquisition, 0.4452), (reo...  \n...                                                 ...  \n9001                                                  0  \n9002                                                  0  \n9003                                                  0  \n9004                                                  0  \n9005                                                  0  \n\n[9006 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>symbol</th>\n      <th>new_symbol</th>\n      <th>name</th>\n      <th>company_word</th>\n      <th>shortName</th>\n      <th>longName</th>\n      <th>company_info</th>\n      <th>finance_info</th>\n      <th>longBusinessSummary</th>\n      <th>longBusinessSummary_keyword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A</td>\n      <td>A</td>\n      <td>Agilent Technologies Inc</td>\n      <td>agilent,technologies</td>\n      <td>agilent technologies  inc</td>\n      <td>Agilent Technologies, Inc.</td>\n      <td>{\"address1\": \"5301 Stevens Creek Boulevard\", \"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>provides application focused solution to the l...</td>\n      <td>[(chromatography, 0.4066), (spectrometry, 0.40...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>AA</td>\n      <td>AA</td>\n      <td>Alcoa Corporation</td>\n      <td>alcoa,corporation</td>\n      <td>alcoa corporation</td>\n      <td>Alcoa Corporation</td>\n      <td>{\"address1\": \"201 Isabella Street\", \"address2\"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>together with it subsidiary produce and sell b...</td>\n      <td>[(alumina, 0.5381), (alcoa, 0.4941), (aluminum...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>Ares Acquisition Corporation</td>\n      <td>ares,acquisition</td>\n      <td>ares acquisition corporation</td>\n      <td>Ares Acquisition Corporation</td>\n      <td>{\"address1\": \"245 Park Avenue\", \"address2\": \"4...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation it intends to effect a m...</td>\n      <td>[(merger, 0.6149), (acquisition, 0.4213), (exc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>AACG</td>\n      <td>AACG</td>\n      <td>ATA Creativity Global</td>\n      <td>ata,creativity</td>\n      <td>ata creativity global</td>\n      <td>ATA Creativity Global</td>\n      <td>{\"address1\": \"Building No. 2\", \"address2\": \"Ea...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>together with it subsidiary provides education...</td>\n      <td>[(ata, 0.4455), (training, 0.3426), (education...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>AACI</td>\n      <td>AACI</td>\n      <td>Armada Acquisition Corp</td>\n      <td>armada,acquisition</td>\n      <td>armada acquisition corp  i</td>\n      <td>Armada Acquisition Corp. I</td>\n      <td>{\"address1\": \"2005 Market Street\", \"address2\":...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>focus on effecting a merger capital stock exch...</td>\n      <td>[(merger, 0.6285), (acquisition, 0.4452), (reo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9001</th>\n      <td>9001</td>\n      <td>ZYME</td>\n      <td>ZYME</td>\n      <td>Zymeworks Inc</td>\n      <td>zymeworks</td>\n      <td>zymeworks inc</td>\n      <td>Zymeworks Inc.</td>\n      <td>{\"address1\": \"114 East 4th Avenue\", \"address2\"...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>discovers develops manufacture and delivers hu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9002</th>\n      <td>9002</td>\n      <td>ZYNE</td>\n      <td>ZYNE</td>\n      <td>Zynerba Pharmaceuticals Inc</td>\n      <td>zynerba,pharmaceuticals</td>\n      <td>zynerba pharmaceuticals  inc</td>\n      <td>Zynerba Pharmaceuticals, Inc.</td>\n      <td>{\"address1\": \"80 West Lancaster Avenue\", \"addr...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>operates a a clinical stage specialty pharmace...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9003</th>\n      <td>9003</td>\n      <td>ZYXI</td>\n      <td>ZYXI</td>\n      <td>Zynex Inc</td>\n      <td>zynex</td>\n      <td>zynex  inc</td>\n      <td>Zynex, Inc.</td>\n      <td>{\"address1\": \"9655 Maroon Circle\", \"city\": \"En...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>through it subsidiary design manufacture and m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9004</th>\n      <td>9004</td>\n      <td>TRUE</td>\n      <td>TRUE</td>\n      <td>TrueCar Inc</td>\n      <td>truecar</td>\n      <td>truecar  inc</td>\n      <td>TrueCar, Inc.</td>\n      <td>{\"address1\": \"120 Broadway\", \"address2\": \"Suit...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation the intends to effect a ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9005</th>\n      <td>9005</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nano labs ltd</td>\n      <td>Nano Labs Ltd</td>\n      <td>{\"address1\": \"Dikaiyinzuo\", \"address2\": \"30th ...</td>\n      <td>{'language': 'en-US', 'region': 'US', 'quoteTy...</td>\n      <td>significant operation the intends to effect a ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9006 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yahoo_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}