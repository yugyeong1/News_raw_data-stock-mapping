{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "news_raw = pd.read_csv(\"C:/projectnasdaq/news_raw.csv\", encoding='latin1') # 뉴스 raw data\n",
    "nasdaq_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks.csv\", encoding='latin1') # stock_list data\n",
    "rep_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks_refine_total.csv\", encoding='latin1') # 대표단어 csv\n",
    "\n",
    "# 테스트 용도로 데이터 1000개\n",
    "news_raw = news_raw.head(1000)\n",
    "news_raw = news_raw[['news_id','title','summary']]\n",
    "nasdaq_stock = nasdaq_stock[['pk','symbol','name']]\n",
    "rep_stock = rep_stock[['pk','symbol','name','name_a']]\n",
    "\n",
    "# new_raw 데이터 전처리\n",
    "news_raw['summary'] = news_raw['summary'].str.lower() #소문자화\n",
    "news_raw['summary'] = news_raw['summary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "self_stop_words = {'bloomberg'}\n",
    "news_raw['summary'].replace(self_stop_words, '', regex=True, inplace=True)\n",
    "\n",
    "# news_raw 데이터 토큰화\n",
    "news_raw['tokenize'] = 0\n",
    "news_raw['tokenize'] = news_raw['summary'].str.split(\" \")\n",
    "news_raw['tokenize'] = news_raw['tokenize'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "news_raw['company_name'] = 0\n",
    "news_raw['keyword'] = 0 # keybert로 뽑아낸 키워드 컬럼\n",
    "news_raw['keyword2'] = 0\n",
    "\n",
    "# list에 담아주기\n",
    "tokenize_list = news_raw['tokenize']\n",
    "rep_list = rep_stock['name_a'].str.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 회사 이름 추출 => 추출 완료 ( company_word.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 회사이름 추출 데이터 저장용도\n",
    "company_word = pd.DataFrame(columns=['news_id', 'company_word','pk'])\n",
    "\n",
    "# 대표단어 csv파일에서 대표단어가 포함되면 맵핑\n",
    "i = 0\n",
    "\n",
    "for token_num, token in enumerate(tokenize_list):  # news_raw data 토큰화\n",
    "    for rep_num, rep in enumerate(rep_list):  # 대표단어 csv파일에서 대표단어에 해당\n",
    "        i = i + 1\n",
    "        if len(rep) == 1:\n",
    "            if rep[0] in token:\n",
    "                company_word.loc[i] = [news_raw['news_id'][token_num], rep[0], rep_stock['pk'][rep_num]]\n",
    "\n",
    "        elif len(rep) == 2:  # 대표단어가 2개로 된 단어일 때\n",
    "            if rep[0] in token:  # 대표단어의 첫번째 단어가 org단어에 있으면\n",
    "                found = token.index(rep[0])  # 대표단어의 첫번째 단어와 일치하는 org단어의 인덱스 위치 번호\n",
    "                try:\n",
    "                    search = found + 1  # stocklist의 첫번째 단어가 org에 포함됐을때 그 다음 단어\n",
    "                    search_found = token[search]  # org의 (+1을 한) 다음 단어에 해당\n",
    "                    if rep[1] == search_found:\n",
    "                        company_word.loc[i] = [news_raw['news_id'][token_num], rep[0] + \" \" + rep[1],\n",
    "                                                 rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        elif len(rep) == 3:\n",
    "            if rep[0] in token:\n",
    "                try:\n",
    "                    search_found = token[token.index(rep[0]) + 1]\n",
    "                    if rep[1] == search_found:\n",
    "                        two_found = token[token.index(rep[0]) + 2]\n",
    "                        if rep[2] == two_found:\n",
    "                            company_word.loc[i] = [news_raw['news_id'][token_num],\n",
    "                                                     rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "company_word = pd.merge(company_word, nasdaq_stock, how='left', left_on='pk', right_on='pk')\n",
    "#company_word = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 원문에서 키워드 추출 ( Keybert 사용 )=> 추출 완료 ( keyword.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keybert로 키워드 추출 ( 키워드 1단어들, 2단어들 )\n",
    "summaries = news_raw['summary']\n",
    "# 뉴스 원문 키워드 10개 추출 저장 컬럼\n",
    "news_raw['news_ten_keyword'] = 0\n",
    "\n",
    "for sum_num, summary in enumerate(summaries):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary,top_n=5)\n",
    "    news_raw['keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1)) # 뉴스 원문 키워드 한 단어들 다섯개 추출\n",
    "    news_raw['keyword2'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 2)) # 뉴스 원문 키워드 두 단어들 다섯개 추출\n",
    "    news_raw['ten_keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10) # 뉴스 원문 키워드 10개 추출"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer 패키지로 단복수 문제 처리\n",
    "news_raw['summary_lemma'] = 0\n",
    "news_raw['tokenize_lemma'] = 0\n",
    "lemma_token = news_raw['tokenize']\n",
    "\n",
    "for i, j in enumerate(lemma_token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    news_raw['tokenize_lemma'][i] = lemma_word\n",
    "    news_raw['summary_lemma'][i] = \" \".join(lemma_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'keyword_lemma'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3360\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'keyword_lemma'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10444\\3620056380.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'keyword_lemma'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m \u001B[1;31m#news_raw.to_csv('C:/projectnasdaq/project2/keyword.csv')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3456\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3457\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3458\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3459\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3460\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3361\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3363\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3365\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mis_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasnans\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'keyword_lemma'"
     ]
    }
   ],
   "source": [
    "summary_lemma = news_raw['summary_lemma']\n",
    "for sum_num, summary in enumerate(summary_lemma):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['keyword_lemma'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))\n",
    "#news_raw.to_csv('C:/projectnasdaq/project2/keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3864\\391799289.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0msummary_lemma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'summary_lemma'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msum_num\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummary\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary_lemma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mkw_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKeyBERT\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mkeywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mnews_raw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'keyword_lemma_ten'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msum_num\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkw_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_keywords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeyphrase_ngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtop_n\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\_model.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m     49\u001B[0m                       \u001B[1;33m*\u001B[0m \u001B[0mhttps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m//\u001B[0m\u001B[0mwww\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msbert\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mpretrained_models\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhtml\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \"\"\"\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect_backend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     def extract_keywords(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_utils.py\u001B[0m in \u001B[0;36mselect_backend\u001B[1;34m(embedding_model)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;31m# Create a Sentence Transformer model based on a string\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mSentenceTransformerBackend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;31m# Hugging Face embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, embedding_model)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0membedding_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSentenceTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m             raise ValueError(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'modules.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m#Load as SentenceTransformer model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_sbert_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m   \u001B[1;31m#Load with AutoModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m                 \u001B[0mmodules\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_auto_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36m_load_sbert_model\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule_config\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodules_config\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    839\u001B[0m             \u001B[0mmodule_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimport_from_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'type'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 840\u001B[1;33m             \u001B[0mmodule\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'path'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    841\u001B[0m             \u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodule_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    842\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(input_path)\u001B[0m\n\u001B[0;32m    135\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msbert_config_path\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfIn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m             \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfIn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtokenizer_name_or_path\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtokenizer_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36m_load_model\u001B[1;34m(self, model_name_or_path, config, cache_dir)\u001B[0m\n\u001B[0;32m     47\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_load_t5_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m             \u001B[0mmodel_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_model_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mmodel_class\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m         raise ValueError(\n\u001B[0;32m    448\u001B[0m             \u001B[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2105\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mContextManagers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minit_contexts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2106\u001B[1;33m             \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2108\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdevice_map\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"auto\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config, add_pooling_layer)\u001B[0m\n\u001B[0;32m    885\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 887\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEmbeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    888\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertEncoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    889\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mposition_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_position_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoken_type_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_vocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_weight\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfactory_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 140\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    141\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_weight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mnum_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36mreset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m         \u001B[0minit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    150\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fill_padding_idx_with_zero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36mnormal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m    153\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhas_torch_function_variadic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtrunc_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m2.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2.\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36m_no_grad_normal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_no_grad_normal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "news_raw['keyword_lemma_ten'] = 0\n",
    "summary_lemma = news_raw['summary_lemma']\n",
    "for sum_num, summary in enumerate(summary_lemma):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['keyword_lemma_ten'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목 정보 수집 ( yfinance 패키지 사용 ) => 수집완료"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 종목코드 리스트\n",
    "stocks = nasdaq_stock['symbol']\n",
    "# 종목코드에 대한 정보 수집용 데이터프레임 생성\n",
    "stock_info = pd.DataFrame(columns=['symbol', 'info'])\n",
    "i = 0\n",
    "\n",
    "for stock_num, stock in enumerate(stocks):\n",
    "    tickers = yf.Ticker(stock)\n",
    "    ticker = tickers.info\n",
    "    i = i + 1\n",
    "    #print(stock_num,stock, \"===>>\",ticker)\n",
    "    stock_info.loc[i] = [stock, ticker]\n",
    "    time.sleep(random.uniform(3, 4))\n",
    "\n",
    "#stock_info.to_csv('C:/projectnasdaq/project2/stock_info.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추출된 회사 이름이랑 ( 전처리 한 ) 뉴스 키워드 연결 ( company_and_keyword.csv 파일에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "company_word = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')\n",
    "keyword = pd.read_csv('C:/projectnasdaq/project2/keyword.csv')\n",
    "# 키워드 전처리\n",
    "keyword['keyword'] = keyword['keyword'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword'] = keyword['keyword'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword'] = keyword['keyword'].str.split(\" \")\n",
    "\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].str.split(\" \")\n",
    "\n",
    "keyword['keyword2'] = keyword['keyword2'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "keyword['keyword2'] = keyword['keyword2'].str.split(\"  \")\n",
    "# 공백으로 토큰화되거나 비어있는 내용으로 토큰화 되어있으면 지우기\n",
    "keyword['keyword2'] = keyword['keyword2'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].str.split(\" \")\n",
    "\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].str.split(\" \")\n",
    "\n",
    "#keyword['summary_lemma'] = 0\n",
    "#keyword['summary_lemma'] = news_raw['summary_lemma']\n",
    "# 위에서 추출해서 csv파일에 summary_lemma부분 저장해둠\n",
    "\n",
    "company_word[['keyword', 'keyword2','keyword_lemma']] = 0\n",
    "\n",
    "keyword_list1 = keyword['keyword']\n",
    "keyword_list2 = keyword['keyword2']\n",
    "\n",
    "keyword_id = keyword['news_id']\n",
    "company_id = company_word['news_id']\n",
    "\n",
    "for i, j in enumerate(company_id):\n",
    "    for k, l in enumerate(keyword_id):\n",
    "        if j == l:\n",
    "            company_word['keyword'][i] = keyword['keyword'][k]\n",
    "            company_word['keyword2'][i] = keyword['keyword2'][k]\n",
    "            company_word['keyword_lemma'][i] = keyword['keyword_lemma'][k]\n",
    "\n",
    "#company_word.to_csv('C:/projectnasdaq/project2/company_and_keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yahoo_dataset 파일 수정 ( yahoo_dataset_mapping.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 파일 수정 ( -> symbol name 수정 )\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset.csv')\n",
    "# yahoo_dataset에서 필요없는 컬럼들 지우고 csv 파일에 있는 종목코드에 종목명 데이터들 붙임\n",
    "\n",
    "yahoo_dataset.insert(1,'new_symbol','0')\n",
    "yahoo_dataset.insert(2,'name','0')\n",
    "yahoo_dataset.insert(3,'company_word','0')\n",
    "\n",
    "yahoo_dataset_symbol = yahoo_dataset['symbol']\n",
    "rep_stock_symbol = rep_stock['symbol']\n",
    "\n",
    "for i, j in enumerate(yahoo_dataset_symbol):\n",
    "    for k, l in enumerate(rep_stock_symbol):\n",
    "        if j == l:\n",
    "            yahoo_dataset['new_symbol'][i] = rep_stock['symbol'][k]\n",
    "            yahoo_dataset['name'][i] = rep_stock['name'][k]\n",
    "            yahoo_dataset['company_word'][i] = rep_stock['name_a'][k]\n",
    "            break\n",
    "\n",
    "# yahoo_dataset 컬럼 정리\n",
    "yahoo_dataset.drop(columns=[\"quoteType\", \"currency\", 'regularMarketPrice', 'regularMarketChange', 'regularMarketChangePercent',\n",
    "             'regularMarketVolume', 'averageDailyVolume3Month', 'marketCap', 'trailingPE', 'fiftyTwoWeekLow',\n",
    "             'fiftyTwoWeekHigh', 'regularMarketOpen', 'priceHint', 'underlyingSymbol'], inplace=True)\n",
    "#yahoo_dataset.to_csv('C:/projectnasdaq/project2/yahoo_dataset_mapping.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목명이 추출된 뉴스 키워드들끼리 모으기 ( 딕셔너리 형태로 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# company단어들 하나씩만 list에 담기 -> 단어 이름이 같은 종목명들의 키워드 모으기 위해서\n",
    "company_and_keyword = pd.read_csv('C:/projectnasdaq/project2/company_and_keyword.csv')\n",
    "company_and_keyword.drop(columns=[\"Unnamed: 0\", 'Unnamed: 0.1'], inplace=True)\n",
    "company_word = company_and_keyword['company_word']\n",
    "\n",
    "company_word_same_list = []\n",
    "for company in company_word:\n",
    "    if company not in company_word_same_list:\n",
    "        company_word_same_list.append(company)\n",
    "\n",
    "company_word_sames = pd.DataFrame(company_word_same_list, columns=['company_word_same'])\n",
    "company_word_same = company_word_sames['company_word_same']\n",
    "\n",
    "# 키워드랑 company_name이랑 데이터프레임에서 맵핑 시키기위한 새로운 dataframe 생성\n",
    "company_name_keyword_organize = pd.DataFrame(columns=['company', 'keyword', 'keyword2','key_list','keyword_lemma'])\n",
    "\n",
    "# 딕셔너리를 { news_id : keyword } 형태로 만들기 위해서 index를 news_id로 설정\n",
    "company_and_keyword = company_and_keyword.set_index(keys='news_id', drop=False, inplace=False)\n",
    "\n",
    "# company_name에 대한 키워드들 딕셔너리 형태로 모으기\n",
    "for i, j in enumerate(company_word_same):\n",
    "    company_name_keyword_organize.loc[i] = [j, company_and_keyword[company_and_keyword['company_word'] == j]['keyword'].to_dict(), company_and_keyword[company_and_keyword['company_word'] == j]['keyword2'].to_dict(),company_and_keyword[company_and_keyword['company_word'] == j]['keyword'].tolist(), company_and_keyword[company_and_keyword['company_word']==j]['keyword_lemma'].tolist()] # 리스트 형태로 키워드만 뽑아오기 위해서"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                 company                                            keyword  \\\n0              coca cola  {'345585': '['nestle', 'starbucks', 'brands', ...   \n1               unilever  {'345585': '['nestle', 'starbucks', 'brands', ...   \n2                 diageo  {'345585': '['nestle', 'starbucks', 'brands', ...   \n3         anheuser busch  {'345585': '['nestle', 'starbucks', 'brands', ...   \n4              starbucks  {'345585': '['nestle', 'starbucks', 'brands', ...   \n..                   ...                                                ...   \n425            delta air  {'443844': '['pandemic', 'airline', 'airlines'...   \n426      goodrx holdings  {'443702': '['recession', 'stocks', 'stocksthe...   \n427           spi energy  {'443710': '['tesla', 'ev', 'spi', 'stocks', '...   \n428  petroleo brasileiro  {'443728': '['vitol', 'indictment', 'prosecuto...   \n429           braskem sa  {'443728': '['vitol', 'indictment', 'prosecuto...   \n\n                                              keyword2  \\\n0    {'345585': '['nestle sa', 'nestle', 'underperf...   \n1    {'345585': '['nestle sa', 'nestle', 'underperf...   \n2    {'345585': '['nestle sa', 'nestle', 'underperf...   \n3    {'345585': '['nestle sa', 'nestle', 'underperf...   \n4    {'345585': '['nestle sa', 'nestle', 'underperf...   \n..                                                 ...   \n425  {'443844': '['airline inevitable', 'pandemic c...   \n426  {'443702': '['rampant stock', ' stocks slumped...   \n427  {'443710': '['energy stocks', ' spi stock', 'i...   \n428  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n429  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n\n                                              key_list  \\\n0    [['nestle', 'starbucks', 'brands', 'bottled', ...   \n1    [['nestle', 'starbucks', 'brands', 'bottled', ...   \n2    [['nestle', 'starbucks', 'brands', 'bottled', ...   \n3    [['nestle', 'starbucks', 'brands', 'bottled', ...   \n4    [['nestle', 'starbucks', 'brands', 'bottled', ...   \n..                                                 ...   \n425  [['pandemic', 'airline', 'airlines', 'coronavi...   \n426  [['recession', 'stocks', 'stocksthe', 'markets...   \n427        [['tesla', 'ev', 'spi', 'stocks', 'stock']]   \n428  [['vitol', 'indictment', 'prosecutors', 'indic...   \n429  [['vitol', 'indictment', 'prosecutors', 'indic...   \n\n                                         keyword_lemma  \n0    [['nestle', 'starbucks', 'bottled', 'market', ...  \n1    [['nestle', 'starbucks', 'bottled', 'market', ...  \n2    [['nestle', 'starbucks', 'bottled', 'market', ...  \n3    [['nestle', 'starbucks', 'bottled', 'market', ...  \n4    [['nestle', 'starbucks', 'bottled', 'market', ...  \n..                                                 ...  \n425  [['vaccine', 'vaccination', 'vaccinated', 'inj...  \n426  [['argentina', 'yield', 'bond', 'buenos', 'tre...  \n427  [['mizuho', 'banking', 'headquarters', 'jpmorg...  \n428  [['kuwait', 'kuwaiti', 'fiscal', 'debt', 'defi...  \n429  [['kuwait', 'kuwaiti', 'fiscal', 'debt', 'defi...  \n\n[430 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>keyword</th>\n      <th>keyword2</th>\n      <th>key_list</th>\n      <th>keyword_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coca cola</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[['nestle', 'starbucks', 'brands', 'bottled', ...</td>\n      <td>[['nestle', 'starbucks', 'bottled', 'market', ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>unilever</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[['nestle', 'starbucks', 'brands', 'bottled', ...</td>\n      <td>[['nestle', 'starbucks', 'bottled', 'market', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>diageo</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[['nestle', 'starbucks', 'brands', 'bottled', ...</td>\n      <td>[['nestle', 'starbucks', 'bottled', 'market', ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>anheuser busch</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[['nestle', 'starbucks', 'brands', 'bottled', ...</td>\n      <td>[['nestle', 'starbucks', 'bottled', 'market', ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>starbucks</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[['nestle', 'starbucks', 'brands', 'bottled', ...</td>\n      <td>[['nestle', 'starbucks', 'bottled', 'market', ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>delta air</td>\n      <td>{'443844': '['pandemic', 'airline', 'airlines'...</td>\n      <td>{'443844': '['airline inevitable', 'pandemic c...</td>\n      <td>[['pandemic', 'airline', 'airlines', 'coronavi...</td>\n      <td>[['vaccine', 'vaccination', 'vaccinated', 'inj...</td>\n    </tr>\n    <tr>\n      <th>426</th>\n      <td>goodrx holdings</td>\n      <td>{'443702': '['recession', 'stocks', 'stocksthe...</td>\n      <td>{'443702': '['rampant stock', ' stocks slumped...</td>\n      <td>[['recession', 'stocks', 'stocksthe', 'markets...</td>\n      <td>[['argentina', 'yield', 'bond', 'buenos', 'tre...</td>\n    </tr>\n    <tr>\n      <th>427</th>\n      <td>spi energy</td>\n      <td>{'443710': '['tesla', 'ev', 'spi', 'stocks', '...</td>\n      <td>{'443710': '['energy stocks', ' spi stock', 'i...</td>\n      <td>[['tesla', 'ev', 'spi', 'stocks', 'stock']]</td>\n      <td>[['mizuho', 'banking', 'headquarters', 'jpmorg...</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>petroleo brasileiro</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[['vitol', 'indictment', 'prosecutors', 'indic...</td>\n      <td>[['kuwait', 'kuwaiti', 'fiscal', 'debt', 'defi...</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>braskem sa</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[['vitol', 'indictment', 'prosecutors', 'indic...</td>\n      <td>[['kuwait', 'kuwaiti', 'fiscal', 'debt', 'defi...</td>\n    </tr>\n  </tbody>\n</table>\n<p>430 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name_keyword_organize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## news_raw_dataset 만들기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# company_name_keyword_organize.csv에 column 삽입\n",
    "company_name_keyword_organize.insert(0, 'symbol', '0')\n",
    "company_name_keyword_organize.insert(1, 'name', '0')\n",
    "company_name_keyword_organize.insert(2, 'shortName', '0')\n",
    "company_name_keyword_organize.insert(3, 'longName', '0')\n",
    "company_name_keyword_organize.insert(5, 'company_info', '0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "yahoo_dataset_mapping = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset_mapping.csv')\n",
    "company_word_csv = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')\n",
    "\n",
    "# company_name_keyword_organize.csv에 symbol, name 데이터 삽입\n",
    "company_word = company_word_csv['company_word']\n",
    "company = company_name_keyword_organize['company']\n",
    "\n",
    "for i,j in enumerate(company_word):\n",
    "    for k, l in enumerate(company):\n",
    "        if j==l:\n",
    "            company_name_keyword_organize['symbol'][k] = company_word_csv['symbol'][i]\n",
    "            company_name_keyword_organize['name'][k] = company_word_csv['name'][i]\n",
    "            break\n",
    "\n",
    "# company__name.csv에 yahoo finance 정보 수집 내용 삽입\n",
    "yahoo_dataset_mapping_symbol = yahoo_dataset_mapping['new_symbol']\n",
    "company_name_keyword_organize_symbol = company_name_keyword_organize['symbol']\n",
    "\n",
    "for i,j in enumerate(yahoo_dataset_mapping_symbol):\n",
    "    for k,l in enumerate(company_name_keyword_organize_symbol):\n",
    "        if j==l:\n",
    "            company_name_keyword_organize['shortName'][k] = yahoo_dataset_mapping['shortName'][i]\n",
    "            company_name_keyword_organize['longName'][k] = yahoo_dataset_mapping['longName'][i]\n",
    "            company_name_keyword_organize['company_info'][k] = yahoo_dataset_mapping['company_info'][i]\n",
    "            break\n",
    "\n",
    "# 대표 키워드 추출하기\n",
    "# 키워드 빈도수 순으로 정리해서 5개까지만 뽑아오기\n",
    "company_name_keyword_organize['frequency_4'] = 0\n",
    "company_name_keyword_organize['frequency_5'] = 0\n",
    "# lemmatize 용도 컬럼 생성\n",
    "company_name_keyword_organize['frequency_4_lemma'] = 0\n",
    "company_name_keyword_organize['frequency_5_lemma'] = 0\n",
    "\n",
    "\n",
    "key_list_ = company_name_keyword_organize['key_list']\n",
    "for i, j in enumerate(key_list_):\n",
    "    company_name_keyword_organize['key_list'][i] = ''.join(j)\n",
    "\n",
    "keyword_lemma_ = company_name_keyword_organize['keyword_lemma']\n",
    "for i,j in enumerate(keyword_lemma_):\n",
    "    company_name_keyword_organize['keyword_lemma'][i] = ''.join(j)\n",
    "\n",
    "company_name_keyword_organize['key_list'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "company_name_keyword_organize['key_list'].replace(',',' ',regex=True, inplace=True)\n",
    "company_name_keyword_organize['key_list'] = company_name_keyword_organize['key_list'].str.split(\" \")\n",
    "company_name_keyword_organize['key_list'] = company_name_keyword_organize['key_list'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "company_name_keyword_organize['keyword_lemma'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "company_name_keyword_organize['keyword_lemma'].replace(',',' ',regex=True, inplace=True)\n",
    "company_name_keyword_organize['keyword_lemma'] = company_name_keyword_organize['keyword_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['keyword_lemma'] = company_name_keyword_organize['keyword_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "\n",
    "\n",
    "frequency_word = company_name_keyword_organize['key_list']\n",
    "for i, j in enumerate(frequency_word):\n",
    "    # 대표 키워드 단어 빈도 순으로 4개 추출\n",
    "    company_name_keyword_organize['frequency_4'][i] = Counter(j).most_common(4)\n",
    "    # 대표 키워드 단어 빈도 순으로 5개 추출\n",
    "    company_name_keyword_organize['frequency_5'][i] = Counter(j).most_common(5)\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# lemmatize 패키지 사용한 단어들로 대표 키워드 빈도 순으로 4-5개 각각 뽑아오기\n",
    "frequency_word_lemma = company_name_keyword_organize['keyword_lemma']\n",
    "for i,j in enumerate(frequency_word_lemma):\n",
    "    company_name_keyword_organize['frequency_4_lemma'][i] = Counter(j).most_common(4)\n",
    "    company_name_keyword_organize['frequency_5_lemma'][i] = Counter(j).most_common(5)\n",
    "\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# 완성된 news_raw dataset\n",
    "#company_name_keyword_organize.to_csv('C:/projectnasdaq/project2/news_raw_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) 뉴스 원문에 종목명의 대표 키워드가 포함될 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑\n",
    "frequency_4 = company_name_keyword_organize['frequency_4'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "news_stock_mapping = pd.DataFrame(columns=['news_id', 'symbol','name','summary','frequency_4'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(tokenize_list):\n",
    "    for k,l in enumerate(frequency_4):\n",
    "        a = a + 1\n",
    "        #print(news_raw['news_id'][i])\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개 이상 교집합으로 있을 경우\n",
    "        if len(together)==4:\n",
    "            news_stock_mapping.loc[a] = [news_raw['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],news_raw['summary'][i], company_name_keyword_organize['frequency_4'][k]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 맵핑 수 : 1290\n"
     ]
    }
   ],
   "source": [
    "print('총 맵핑 수 :',len(news_stock_mapping))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑(2) 뉴스 원문 키워드(10개)랑 종목명의 대표 키워드가 일치할 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "news_stock_mapping_2 = pd.DataFrame(columns=['news_id', 'symbol','name','summary','frequency_5','news_keyword_ten'])\n",
    "news_raw_keyword_ten = keyword['news_keyword_ten']\n",
    "frequency_5 = company_name_keyword_organize['frequency_5']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_raw_keyword_ten):\n",
    "    for k,l in enumerate(frequency_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            news_stock_mapping_2.loc[a] = [keyword['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],keyword['summary'][i], company_name_keyword_organize['frequency_5'][k],keyword['news_keyword_ten'][i]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 맵핑 수 : 1069\n"
     ]
    }
   ],
   "source": [
    "print('총 맵핑 수 :',len(news_stock_mapping_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "news_stock_mapping = news_stock_mapping.drop(columns=['frequency_4'])\n",
    "news_stock_mapping_2 = news_stock_mapping_2.drop(columns=['frequency_5', 'news_keyword_ten'])\n",
    "news_stock_mapping_final = pd.concat([news_stock_mapping,news_stock_mapping_2], ignore_index=True)\n",
    "overlap = news_stock_mapping_final.drop_duplicates(['news_id','symbol'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두가지 방법으로 맵핑했을 때 최종 맵핑 결과 : 1598\n"
     ]
    }
   ],
   "source": [
    "print('두가지 방법으로 맵핑했을 때 최종 맵핑 결과 :' ,len(overlap))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "data": {
      "text/plain": "    symbol                                             name  \\\n0       KO                          Coca-Cola Company (The)   \n1       UL                                     Unilever PLC   \n2      DEO                                       Diageo plc   \n3      BUD  Anheuser-Busch Inbev SA Sponsored ADR (Belgium)   \n4     SBUX                            Starbucks Corporation   \n..     ...                                              ...   \n425    DAL                              Delta Air Lines Inc   \n426   GDRX                              GoodRx Holdings Inc   \n427    SPI                                    SPI Energy Co   \n428    PBR                            Petroleo Brasileiro S   \n429    BAK                                   Braskem SA ADR   \n\n                           shortName                              longName  \\\n0            Coca-Cola Company (The)                 The Coca-Cola Company   \n1                       Unilever PLC                          Unilever PLC   \n2                         Diageo plc                            Diageo plc   \n3    Anheuser-Busch Inbev SA Sponsor            Anheuser-Busch InBev SA/NV   \n4              Starbucks Corporation                 Starbucks Corporation   \n..                               ...                                   ...   \n425            Delta Air Lines, Inc.                 Delta Air Lines, Inc.   \n426            GoodRx Holdings, Inc.                 GoodRx Holdings, Inc.   \n427             SPI Energy Co., Ltd.                  SPI Energy Co., Ltd.   \n428  Petroleo Brasileiro S.A.- Petro  Petróleo Brasileiro S.A. - Petrobras   \n429                       Braskem SA                          Braskem S.A.   \n\n                 company                                       company_info  \\\n0              coca cola  {\"address1\": \"One Coca-Cola Plaza\", \"city\": \"A...   \n1               unilever  {\"address1\": \"Unilever House\", \"address2\": \"10...   \n2                 diageo  {\"address1\": \"Lakeside Drive\", \"address2\": \"Pa...   \n3         anheuser busch  {\"address1\": \"Brouwerijplein 1\", \"city\": \"Leuv...   \n4              starbucks  {\"address1\": \"2401 Utah Avenue South\", \"city\":...   \n..                   ...                                                ...   \n425            delta air  {\"address1\": \"PO Box 20706\", \"city\": \"Atlanta\"...   \n426      goodrx holdings  {\"address1\": \"2701 Olympic Boulevard\", \"addres...   \n427           spi energy  {\"address1\": \"4803 Urbani Avenue\", \"address2\":...   \n428  petroleo brasileiro  {\"address1\": \"Avenida RepUblica do Chile, 65\",...   \n429           braskem sa  {\"address1\": \"120, Rua Lemos Monteiro\", \"addre...   \n\n                                               keyword  \\\n0    {'345585': '['nestle', 'starbucks', 'brands', ...   \n1    {'345585': '['nestle', 'starbucks', 'brands', ...   \n2    {'345585': '['nestle', 'starbucks', 'brands', ...   \n3    {'345585': '['nestle', 'starbucks', 'brands', ...   \n4    {'345585': '['nestle', 'starbucks', 'brands', ...   \n..                                                 ...   \n425  {'443844': '['pandemic', 'airline', 'airlines'...   \n426  {'443702': '['recession', 'stocks', 'stocksthe...   \n427  {'443710': '['tesla', 'ev', 'spi', 'stocks', '...   \n428  {'443728': '['vitol', 'indictment', 'prosecuto...   \n429  {'443728': '['vitol', 'indictment', 'prosecuto...   \n\n                                              keyword2  \\\n0    {'345585': '['nestle sa', 'nestle', 'underperf...   \n1    {'345585': '['nestle sa', 'nestle', 'underperf...   \n2    {'345585': '['nestle sa', 'nestle', 'underperf...   \n3    {'345585': '['nestle sa', 'nestle', 'underperf...   \n4    {'345585': '['nestle sa', 'nestle', 'underperf...   \n..                                                 ...   \n425  {'443844': '['airline inevitable', 'pandemic c...   \n426  {'443702': '['rampant stock', ' stocks slumped...   \n427  {'443710': '['energy stocks', ' spi stock', 'i...   \n428  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n429  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n\n                                              key_list  \\\n0    [nestle, starbucks, brands, bottled, marketpos...   \n1    [nestle, starbucks, brands, bottled, marketpal...   \n2    [nestle, starbucks, brands, bottled, marketdia...   \n3    [nestle, starbucks, brands, bottled, marketinv...   \n4    [nestle, starbucks, brands, bottled, marketdis...   \n..                                                 ...   \n425  [pandemic, airline, airlines, coronavirus, fli...   \n426     [recession, stocks, stocksthe, markets, stock]   \n427                    [tesla, ev, spi, stocks, stock]   \n428  [vitol, indictment, prosecutors, indicted, pro...   \n429  [vitol, indictment, prosecutors, indicted, pro...   \n\n                                         keyword_lemma  \\\n0    [nestle, starbucks, bottled, market, colapostm...   \n1    [nestle, starbucks, bottled, market, coladefor...   \n2    [nestle, starbucks, bottled, market, coladiage...   \n3    [nestle, starbucks, bottled, market, colainves...   \n4    [nestle, starbucks, bottled, market, coladiscr...   \n..                                                 ...   \n425  [vaccine, vaccination, vaccinated, injection, ...   \n426         [argentina, yield, bond, buenos, treasury]   \n427  [mizuho, banking, headquarters, jpmorgan, office]   \n428           [kuwait, kuwaiti, fiscal, debt, deficit]   \n429           [kuwait, kuwaiti, fiscal, debt, deficit]   \n\n                                    frequency_4  \\\n0             [cola, nestle, starbucks, brands]   \n1          [nestle, starbucks, brands, bottled]   \n2          [nestle, starbucks, brands, bottled]   \n3          [nestle, starbucks, brands, bottled]   \n4         [starbucks, shenzhen, nestle, brands]   \n..                                          ...   \n425  [pandemic, airline, airlines, coronavirus]   \n426     [recession, stocks, stocksthe, markets]   \n427                    [tesla, ev, spi, stocks]   \n428  [vitol, indictment, prosecutors, indicted]   \n429  [vitol, indictment, prosecutors, indicted]   \n\n                                           frequency_5  \\\n0           [cola, nestle, starbucks, brands, bottled]   \n1     [nestle, starbucks, brands, bottled, marketpalm]   \n2    [nestle, starbucks, brands, bottled, marketdia...   \n3    [nestle, starbucks, brands, bottled, marketinv...   \n4       [starbucks, shenzhen, nestle, brands, bottled]   \n..                                                 ...   \n425  [pandemic, airline, airlines, coronavirus, fli...   \n426     [recession, stocks, stocksthe, markets, stock]   \n427                    [tesla, ev, spi, stocks, stock]   \n428  [vitol, indictment, prosecutors, indicted, pro...   \n429  [vitol, indictment, prosecutors, indicted, pro...   \n\n                                 frequency_4_lemma  \\\n0             [nestle, starbucks, bottled, market]   \n1             [nestle, starbucks, bottled, market]   \n2            [company, nestle, starbucks, bottled]   \n3             [nestle, starbucks, bottled, market]   \n4         [financing, shenzhen, nestle, starbucks]   \n..                                             ...   \n425  [vaccine, vaccination, vaccinated, injection]   \n426               [argentina, yield, bond, buenos]   \n427      [mizuho, banking, headquarters, jpmorgan]   \n428                [kuwait, kuwaiti, fiscal, debt]   \n429                [kuwait, kuwaiti, fiscal, debt]   \n\n                                     frequency_5_lemma  \n0    [nestle, starbucks, bottled, market, colapostm...  \n1    [nestle, starbucks, bottled, market, coladefor...  \n2        [company, nestle, starbucks, bottled, market]  \n3    [nestle, starbucks, bottled, market, colainves...  \n4    [financing, shenzhen, nestle, starbucks, bottled]  \n..                                                 ...  \n425  [vaccine, vaccination, vaccinated, injection, ...  \n426         [argentina, yield, bond, buenos, treasury]  \n427  [mizuho, banking, headquarters, jpmorgan, office]  \n428           [kuwait, kuwaiti, fiscal, debt, deficit]  \n429           [kuwait, kuwaiti, fiscal, debt, deficit]  \n\n[430 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>shortName</th>\n      <th>longName</th>\n      <th>company</th>\n      <th>company_info</th>\n      <th>keyword</th>\n      <th>keyword2</th>\n      <th>key_list</th>\n      <th>keyword_lemma</th>\n      <th>frequency_4</th>\n      <th>frequency_5</th>\n      <th>frequency_4_lemma</th>\n      <th>frequency_5_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KO</td>\n      <td>Coca-Cola Company (The)</td>\n      <td>Coca-Cola Company (The)</td>\n      <td>The Coca-Cola Company</td>\n      <td>coca cola</td>\n      <td>{\"address1\": \"One Coca-Cola Plaza\", \"city\": \"A...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketpos...</td>\n      <td>[nestle, starbucks, bottled, market, colapostm...</td>\n      <td>[cola, nestle, starbucks, brands]</td>\n      <td>[cola, nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, colapostm...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UL</td>\n      <td>Unilever PLC</td>\n      <td>Unilever PLC</td>\n      <td>Unilever PLC</td>\n      <td>unilever</td>\n      <td>{\"address1\": \"Unilever House\", \"address2\": \"10...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketpal...</td>\n      <td>[nestle, starbucks, bottled, market, coladefor...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketpalm]</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, coladefor...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>Diageo plc</td>\n      <td>Diageo plc</td>\n      <td>diageo</td>\n      <td>{\"address1\": \"Lakeside Drive\", \"address2\": \"Pa...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketdia...</td>\n      <td>[nestle, starbucks, bottled, market, coladiage...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketdia...</td>\n      <td>[company, nestle, starbucks, bottled]</td>\n      <td>[company, nestle, starbucks, bottled, market]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BUD</td>\n      <td>Anheuser-Busch Inbev SA Sponsored ADR (Belgium)</td>\n      <td>Anheuser-Busch Inbev SA Sponsor</td>\n      <td>Anheuser-Busch InBev SA/NV</td>\n      <td>anheuser busch</td>\n      <td>{\"address1\": \"Brouwerijplein 1\", \"city\": \"Leuv...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketinv...</td>\n      <td>[nestle, starbucks, bottled, market, colainves...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketinv...</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, colainves...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SBUX</td>\n      <td>Starbucks Corporation</td>\n      <td>Starbucks Corporation</td>\n      <td>Starbucks Corporation</td>\n      <td>starbucks</td>\n      <td>{\"address1\": \"2401 Utah Avenue South\", \"city\":...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketdis...</td>\n      <td>[nestle, starbucks, bottled, market, coladiscr...</td>\n      <td>[starbucks, shenzhen, nestle, brands]</td>\n      <td>[starbucks, shenzhen, nestle, brands, bottled]</td>\n      <td>[financing, shenzhen, nestle, starbucks]</td>\n      <td>[financing, shenzhen, nestle, starbucks, bottled]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>DAL</td>\n      <td>Delta Air Lines Inc</td>\n      <td>Delta Air Lines, Inc.</td>\n      <td>Delta Air Lines, Inc.</td>\n      <td>delta air</td>\n      <td>{\"address1\": \"PO Box 20706\", \"city\": \"Atlanta\"...</td>\n      <td>{'443844': '['pandemic', 'airline', 'airlines'...</td>\n      <td>{'443844': '['airline inevitable', 'pandemic c...</td>\n      <td>[pandemic, airline, airlines, coronavirus, fli...</td>\n      <td>[vaccine, vaccination, vaccinated, injection, ...</td>\n      <td>[pandemic, airline, airlines, coronavirus]</td>\n      <td>[pandemic, airline, airlines, coronavirus, fli...</td>\n      <td>[vaccine, vaccination, vaccinated, injection]</td>\n      <td>[vaccine, vaccination, vaccinated, injection, ...</td>\n    </tr>\n    <tr>\n      <th>426</th>\n      <td>GDRX</td>\n      <td>GoodRx Holdings Inc</td>\n      <td>GoodRx Holdings, Inc.</td>\n      <td>GoodRx Holdings, Inc.</td>\n      <td>goodrx holdings</td>\n      <td>{\"address1\": \"2701 Olympic Boulevard\", \"addres...</td>\n      <td>{'443702': '['recession', 'stocks', 'stocksthe...</td>\n      <td>{'443702': '['rampant stock', ' stocks slumped...</td>\n      <td>[recession, stocks, stocksthe, markets, stock]</td>\n      <td>[argentina, yield, bond, buenos, treasury]</td>\n      <td>[recession, stocks, stocksthe, markets]</td>\n      <td>[recession, stocks, stocksthe, markets, stock]</td>\n      <td>[argentina, yield, bond, buenos]</td>\n      <td>[argentina, yield, bond, buenos, treasury]</td>\n    </tr>\n    <tr>\n      <th>427</th>\n      <td>SPI</td>\n      <td>SPI Energy Co</td>\n      <td>SPI Energy Co., Ltd.</td>\n      <td>SPI Energy Co., Ltd.</td>\n      <td>spi energy</td>\n      <td>{\"address1\": \"4803 Urbani Avenue\", \"address2\":...</td>\n      <td>{'443710': '['tesla', 'ev', 'spi', 'stocks', '...</td>\n      <td>{'443710': '['energy stocks', ' spi stock', 'i...</td>\n      <td>[tesla, ev, spi, stocks, stock]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan, office]</td>\n      <td>[tesla, ev, spi, stocks]</td>\n      <td>[tesla, ev, spi, stocks, stock]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan, office]</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>PBR</td>\n      <td>Petroleo Brasileiro S</td>\n      <td>Petroleo Brasileiro S.A.- Petro</td>\n      <td>Petróleo Brasileiro S.A. - Petrobras</td>\n      <td>petroleo brasileiro</td>\n      <td>{\"address1\": \"Avenida RepUblica do Chile, 65\",...</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n      <td>[vitol, indictment, prosecutors, indicted]</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt]</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>BAK</td>\n      <td>Braskem SA ADR</td>\n      <td>Braskem SA</td>\n      <td>Braskem S.A.</td>\n      <td>braskem sa</td>\n      <td>{\"address1\": \"120, Rua Lemos Monteiro\", \"addre...</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n      <td>[vitol, indictment, prosecutors, indicted]</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt]</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n    </tr>\n  </tbody>\n</table>\n<p>430 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name_keyword_organize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) - (Lemmatizer) 뉴스 원문에 종목명의 대표 키워드가 포함될 경우"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "    news_id                                              title  \\\n0    345585  Nestle Outshines Rivals With Revenue Growth Le...   \n1    345586  Total Makes Surprise Profit as Trading Gains O...   \n2    345590  Airbus Follows Boeing in Paring Output to Weat...   \n3    345591  Virus Uptick Imperils South Europe's Nascent T...   \n4    345595  PG&E Fire Insurance Costs Skyrocket After Bank...   \n..      ...                                                ...   \n995  443773  Localiza to Form $9 Billion Car-Rental Giant W...   \n996  443778  Mendoza Province Is First to Restructure After...   \n997  443779  Manufacturing Keeps German Economy on a Recove...   \n998  443780  ECB Must Limit Emergency Powers to Temporary C...   \n999  443781  Citi Pledges to Become Antiracist, Review Inte...   \n\n                                               summary  \\\n0         nestle sa sailed past ailing consumer goo...   \n1         total se made a surprise profit after  ve...   \n2         airbus se cut back wide body jet producti...   \n3         nadine scheiner s efforts to travel from ...   \n4         pg&e corp  is finding it very costly to b...   \n..                                                 ...   \n995       localiza rent a car sa and unidas  two of...   \n996       argentina s mendoza province has reached ...   \n997       german factories kept europe s biggest ec...   \n998       the european central bank risks legal tro...   \n999       citigroup inc  will spend  1 billion over...   \n\n                                              tokenize  company_name  keyword  \\\n0    [nestle, sa, sailed, past, ailing, consumer, g...             0        0   \n1    [total, se, made, a, surprise, profit, after, ...             0        0   \n2    [airbus, se, cut, back, wide, body, jet, produ...             0        0   \n3    [nadine, scheiner, s, efforts, to, travel, fro...             0        0   \n4    [pg&e, corp, is, finding, it, very, costly, to...             0        0   \n..                                                 ...           ...      ...   \n995  [localiza, rent, a, car, sa, and, unidas, two,...             0        0   \n996  [argentina, s, mendoza, province, has, reached...             0        0   \n997  [german, factories, kept, europe, s, biggest, ...             0        0   \n998  [the, european, central, bank, risks, legal, t...             0        0   \n999  [citigroup, inc, will, spend, 1, billion, over...             0        0   \n\n     keyword2                                      summary_lemma  \\\n0           0  nestle sa sailed past ailing consumer good riv...   \n1           0  total se made a surprise profit after very goo...   \n2           0  airbus se cut back wide body jet production af...   \n3           0  nadine scheiner s effort to travel from her ho...   \n4           0  pg&e corp is finding it very costly to buy fir...   \n..        ...                                                ...   \n995         0  localiza rent a car sa and unidas two of brazi...   \n996         0  argentina s mendoza province ha reached an agr...   \n997         0  german factory kept europe s biggest economy m...   \n998         0  the european central bank risk legal trouble i...   \n999         0  citigroup inc will spend 1 billion over the ne...   \n\n                                        tokenize_lemma  \n0    [nestle, sa, sailed, past, ailing, consumer, g...  \n1    [total, se, made, a, surprise, profit, after, ...  \n2    [airbus, se, cut, back, wide, body, jet, produ...  \n3    [nadine, scheiner, s, effort, to, travel, from...  \n4    [pg&e, corp, is, finding, it, very, costly, to...  \n..                                                 ...  \n995  [localiza, rent, a, car, sa, and, unidas, two,...  \n996  [argentina, s, mendoza, province, ha, reached,...  \n997  [german, factory, kept, europe, s, biggest, ec...  \n998  [the, european, central, bank, risk, legal, tr...  \n999  [citigroup, inc, will, spend, 1, billion, over...  \n\n[1000 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>tokenize</th>\n      <th>company_name</th>\n      <th>keyword</th>\n      <th>keyword2</th>\n      <th>summary_lemma</th>\n      <th>tokenize_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>345585</td>\n      <td>Nestle Outshines Rivals With Revenue Growth Le...</td>\n      <td>nestle sa sailed past ailing consumer goo...</td>\n      <td>[nestle, sa, sailed, past, ailing, consumer, g...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n      <td>[nestle, sa, sailed, past, ailing, consumer, g...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>345586</td>\n      <td>Total Makes Surprise Profit as Trading Gains O...</td>\n      <td>total se made a surprise profit after  ve...</td>\n      <td>[total, se, made, a, surprise, profit, after, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>total se made a surprise profit after very goo...</td>\n      <td>[total, se, made, a, surprise, profit, after, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345590</td>\n      <td>Airbus Follows Boeing in Paring Output to Weat...</td>\n      <td>airbus se cut back wide body jet producti...</td>\n      <td>[airbus, se, cut, back, wide, body, jet, produ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>airbus se cut back wide body jet production af...</td>\n      <td>[airbus, se, cut, back, wide, body, jet, produ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>345591</td>\n      <td>Virus Uptick Imperils South Europe's Nascent T...</td>\n      <td>nadine scheiner s efforts to travel from ...</td>\n      <td>[nadine, scheiner, s, efforts, to, travel, fro...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nadine scheiner s effort to travel from her ho...</td>\n      <td>[nadine, scheiner, s, effort, to, travel, from...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>345595</td>\n      <td>PG&amp;E Fire Insurance Costs Skyrocket After Bank...</td>\n      <td>pg&amp;e corp  is finding it very costly to b...</td>\n      <td>[pg&amp;e, corp, is, finding, it, very, costly, to...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>pg&amp;e corp is finding it very costly to buy fir...</td>\n      <td>[pg&amp;e, corp, is, finding, it, very, costly, to...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>443773</td>\n      <td>Localiza to Form $9 Billion Car-Rental Giant W...</td>\n      <td>localiza rent a car sa and unidas  two of...</td>\n      <td>[localiza, rent, a, car, sa, and, unidas, two,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>localiza rent a car sa and unidas two of brazi...</td>\n      <td>[localiza, rent, a, car, sa, and, unidas, two,...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>443778</td>\n      <td>Mendoza Province Is First to Restructure After...</td>\n      <td>argentina s mendoza province has reached ...</td>\n      <td>[argentina, s, mendoza, province, has, reached...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>argentina s mendoza province ha reached an agr...</td>\n      <td>[argentina, s, mendoza, province, ha, reached,...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>443779</td>\n      <td>Manufacturing Keeps German Economy on a Recove...</td>\n      <td>german factories kept europe s biggest ec...</td>\n      <td>[german, factories, kept, europe, s, biggest, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>german factory kept europe s biggest economy m...</td>\n      <td>[german, factory, kept, europe, s, biggest, ec...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>443780</td>\n      <td>ECB Must Limit Emergency Powers to Temporary C...</td>\n      <td>the european central bank risks legal tro...</td>\n      <td>[the, european, central, bank, risks, legal, t...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>the european central bank risk legal trouble i...</td>\n      <td>[the, european, central, bank, risk, legal, tr...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>443781</td>\n      <td>Citi Pledges to Become Antiracist, Review Inte...</td>\n      <td>citigroup inc  will spend  1 billion over...</td>\n      <td>[citigroup, inc, will, spend, 1, billion, over...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>citigroup inc will spend 1 billion over the ne...</td>\n      <td>[citigroup, inc, will, spend, 1, billion, over...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문에 대표 키워드가 포함될 경우 508\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑(lemmatize)\n",
    "tokenize_list_lemma = news_raw['tokenize_lemma']\n",
    "\n",
    "frequency_5_lemma = company_name_keyword_organize['frequency_5_lemma'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "news_stock_mapping_lemma = pd.DataFrame(columns=['news_id', 'symbol','name','summary_lemma','frequency_5_lemma'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(tokenize_list_lemma):\n",
    "    for k,l in enumerate(frequency_5_lemma):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개가 교집합으로 있을 경우\n",
    "        if len(together)==5:\n",
    "            news_stock_mapping_lemma.loc[a] = [news_raw['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],news_raw['summary_lemma'][i], company_name_keyword_organize['frequency_5_lemma'][k]]\n",
    "\n",
    "print('뉴스 원문에 대표 키워드가 포함될 경우',len(news_stock_mapping_lemma))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (2) - (Lemmatizer) 뉴스 원문 키워드와 종목명의 대표 키워드의 교집합이 4개 이상일 경우"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 : 974\n"
     ]
    }
   ],
   "source": [
    "news_stock_mapping_2_lemma = pd.DataFrame(columns=['news_id', 'symbol','name','summary_lemma','frequency_5_lemma','news_keyword_lemma_ten'])\n",
    "news_raw_keyword_lemma_ten = keyword['keyword_lemma_ten']\n",
    "frequency_5_lemma = company_name_keyword_organize['frequency_5_lemma']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_raw_keyword_lemma_ten):\n",
    "    for k,l in enumerate(frequency_5_lemma):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            news_stock_mapping_2_lemma.loc[a] = [keyword['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],keyword['summary_lemma'][i], company_name_keyword_organize['frequency_5_lemma'][k],keyword['keyword_lemma_ten'][i]]\n",
    "\n",
    "print('뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 :',len(news_stock_mapping_2_lemma))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "     news_id symbol                              name  \\\n0     345585    DEO                        Diageo plc   \n1     345591   ATSG  Air Transport Services Group Inc   \n2     345620   COST      Costco Wholesale Corporation   \n3     345620     DG        Dollar General Corporation   \n4     345620   BBWI             Bath & Body Works Inc   \n...      ...    ...                               ...   \n1477  443843    LEN                Lennar Corporation   \n1478  443844   GDRX               GoodRx Holdings Inc   \n1479  443702    SPI                     SPI Energy Co   \n1480  443718    PBR             Petroleo Brasileiro S   \n1481  443718    BAK                    Braskem SA ADR   \n\n                                          summary_lemma  \n0     nestle sa sailed past ailing consumer good riv...  \n1     nadine scheiner s effort to travel from her ho...  \n2     walmart inc might be one of the few big winner...  \n3     walmart inc might be one of the few big winner...  \n4     walmart inc might be one of the few big winner...  \n...                                                 ...  \n1477  the hunt for renewable energy stock sent one o...  \n1478  mizuho financial group inc plan to trim office...  \n1479  new zealand s central bank said it may launch ...  \n1480  the economy of the united arab emirate will su...  \n1481  the economy of the united arab emirate will su...  \n\n[1262 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>summary_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>345585</td>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>345591</td>\n      <td>ATSG</td>\n      <td>Air Transport Services Group Inc</td>\n      <td>nadine scheiner s effort to travel from her ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345620</td>\n      <td>COST</td>\n      <td>Costco Wholesale Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>345620</td>\n      <td>DG</td>\n      <td>Dollar General Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>345620</td>\n      <td>BBWI</td>\n      <td>Bath &amp; Body Works Inc</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>443843</td>\n      <td>LEN</td>\n      <td>Lennar Corporation</td>\n      <td>the hunt for renewable energy stock sent one o...</td>\n    </tr>\n    <tr>\n      <th>1478</th>\n      <td>443844</td>\n      <td>GDRX</td>\n      <td>GoodRx Holdings Inc</td>\n      <td>mizuho financial group inc plan to trim office...</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>443702</td>\n      <td>SPI</td>\n      <td>SPI Energy Co</td>\n      <td>new zealand s central bank said it may launch ...</td>\n    </tr>\n    <tr>\n      <th>1480</th>\n      <td>443718</td>\n      <td>PBR</td>\n      <td>Petroleo Brasileiro S</td>\n      <td>the economy of the united arab emirate will su...</td>\n    </tr>\n    <tr>\n      <th>1481</th>\n      <td>443718</td>\n      <td>BAK</td>\n      <td>Braskem SA ADR</td>\n      <td>the economy of the united arab emirate will su...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1262 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_stock_mapping_lemma = news_stock_mapping_lemma.drop(columns=['frequency_5_lemma'])\n",
    "news_stock_mapping_2_lemma = news_stock_mapping_2_lemma.drop(columns=['frequency_5_lemma', 'news_keyword_lemma_ten'])\n",
    "news_stock_mapping_final_lemma = pd.concat([news_stock_mapping_lemma,news_stock_mapping_2_lemma], ignore_index=True)\n",
    "overlap = news_stock_mapping_final_lemma.drop_duplicates(['news_id','symbol'])\n",
    "overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "yahoo_dataset_mapping = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset_mapping.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"address1\": \"245 Park Avenue\", \"address2\": \"44th Floor\", \"city\": \"New York\", \"state\": \"NY\", \"zip\": \"10167\", \"country\": \"United States\", \"phone\": \"310 201 4100\", \"website\": \"https://www.aresacquisitioncorporation.com\", \"industry\": \"Shell Companies\", \"sector\": \"Financial Services\", \"longBusinessSummary\": \"Ares Acquisition Corporation does not have significant operations. It intends to effect a merger, share exchange, asset acquisition, share purchase, reorganization, or similar business combination with one or more businesses. The company was incorporated in 2020 and is based in New York, New York.\", \"companyOfficers\": [{\"maxAge\": 1, \"name\": \"Mr. David B. Kaplan\", \"age\": 54, \"title\": \"CEO & Co-Chairman\", \"yearBorn\": 1967, \"exercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}, \"unexercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}}, {\"maxAge\": 1, \"name\": \"Mr. Jarrod Morgan Phillips\", \"age\": 42, \"title\": \"Chief Financial Officer\", \"yearBorn\": 1979, \"exercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}, \"unexercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}}, {\"maxAge\": 1, \"name\": \"Ms. Allyson  Satin\", \"age\": 35, \"title\": \"Chief Operating Officer\", \"yearBorn\": 1986, \"exercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}, \"unexercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}}, {\"maxAge\": 1, \"name\": \"Mr. Peter  Ogilvie\", \"age\": 37, \"title\": \"Exec. VP of Strategy\", \"yearBorn\": 1984, \"exercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}, \"unexercisedValue\": {\"raw\": 0, \"fmt\": null, \"longFmt\": \"0\"}}], \"maxAge\": 86400}'"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = yahoo_dataset_mapping['company_info'][2]\n",
    "str"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "str = yahoo_dataset_mapping['company_info'][4]\n",
    "name = yahoo_dataset_mapping['shortName'][4]\n",
    "keys =[]\n",
    "values = []\n",
    "data_list = str.split(', \"')\n",
    "for data in data_list:\n",
    "    pair = data.split('\":')\n",
    "    keys.append(pair[0])\n",
    "    values.append(pair[1])\n",
    "\n",
    "my_dict = dict(zip(keys, values))\n",
    "a= my_dict.get('longBusinessSummary')\n",
    "a = a.replace(name,\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "'Armada Acquisition Corp. I'"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" focuses on effecting a merger, capital stock exchange, asset acquisition, stock purchase, reorganization, or related business combination with one or more businesses. It intends to focus on target businesses that provide technological services to the financial services industry. The company was incorporated in 2020 and is based in Philadelphia, Pennsylvania.\"\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('incorporated', 0.5225), ('merger', 0.479), ('businesses', 0.4049), ('company', 0.3922), ('capital', 0.3802)]\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(a,top_n=5)\n",
    "print(keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}