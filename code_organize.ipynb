{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "news_raw = pd.read_csv(\"C:/projectnasdaq/news_raw.csv\", encoding='latin1') # 뉴스 raw data\n",
    "nasdaq_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks.csv\", encoding='latin1') # stock_list data\n",
    "rep_stock = pd.read_csv(\"C:/projectnasdaq/nasdaq_stocks_refine_total.csv\", encoding='latin1') # 대표단어 csv\n",
    "\n",
    "# 테스트 용도로 데이터 1000개만 가지고\n",
    "news_raw = news_raw.head(1000)\n",
    "news_raw = news_raw[['news_id','title','summary']]\n",
    "nasdaq_stock = nasdaq_stock[['pk','symbol','name']]\n",
    "rep_stock = rep_stock[['pk','symbol','name','name_a']]\n",
    "\n",
    "# new_raw 데이터 전처리\n",
    "news_raw['summary'] = news_raw['summary'].str.lower() #소문자화\n",
    "news_raw['summary'] = news_raw['summary'].apply(lambda x: re.sub('[^a-zA-Z\\d&]', ' ', str(x)).strip())\n",
    "self_stop_words = {'bloomberg'}\n",
    "news_raw['summary'].replace(self_stop_words, '', regex=True, inplace=True)\n",
    "\n",
    "# news_raw 데이터 토큰화\n",
    "news_raw['tokenize'] = 0\n",
    "news_raw['tokenize'] = news_raw['summary'].str.split(\" \")\n",
    "news_raw['tokenize'] = news_raw['tokenize'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "news_raw['company_name'] = 0\n",
    "news_raw['keyword'] = 0 # keybert로 뽑아낸 키워드 컬럼\n",
    "news_raw['keyword2'] = 0\n",
    "\n",
    "# list에 담아주기\n",
    "tokenize_list = news_raw['tokenize']\n",
    "rep_list = rep_stock['name_a'].str.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 회사 이름 추출 => 추출 완료 ( company_word.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 회사이름 추출 데이터 저장용도\n",
    "company_word = pd.DataFrame(columns=['news_id', 'company_word','pk'])\n",
    "\n",
    "# 대표단어 csv파일에서 대표단어가 포함되면 맵핑\n",
    "i = 0\n",
    "\n",
    "for token_num, token in enumerate(tokenize_list):  # news_raw data 토큰화\n",
    "    for rep_num, rep in enumerate(rep_list):  # 대표단어 csv파일에서 대표단어에 해당\n",
    "        i = i + 1\n",
    "        if len(rep) == 1:\n",
    "            if rep[0] in token:\n",
    "                company_word.loc[i] = [news_raw['news_id'][token_num], rep[0], rep_stock['pk'][rep_num]]\n",
    "\n",
    "        elif len(rep) == 2:  # 대표단어가 2개로 된 단어일 때\n",
    "            if rep[0] in token:  # 대표단어의 첫번째 단어가 org단어에 있으면\n",
    "                found = token.index(rep[0])  # 대표단어의 첫번째 단어와 일치하는 org단어의 인덱스 위치 번호\n",
    "                try:\n",
    "                    search = found + 1  # stocklist의 첫번째 단어가 org에 포함됐을때 그 다음 단어\n",
    "                    search_found = token[search]  # org의 (+1을 한) 다음 단어에 해당\n",
    "                    if rep[1] == search_found:\n",
    "                        company_word.loc[i] = [news_raw['news_id'][token_num], rep[0] + \" \" + rep[1],\n",
    "                                                 rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        elif len(rep) == 3:\n",
    "            if rep[0] in token:\n",
    "                try:\n",
    "                    search_found = token[token.index(rep[0]) + 1]\n",
    "                    if rep[1] == search_found:\n",
    "                        two_found = token[token.index(rep[0]) + 2]\n",
    "                        if rep[2] == two_found:\n",
    "                            company_word.loc[i] = [news_raw['news_id'][token_num],\n",
    "                                                     rep[0] + \" \" + rep[1] + \" \" + rep[2], rep_stock['pk'][rep_num]]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "company_word = pd.merge(company_word, nasdaq_stock, how='left', left_on='pk', right_on='pk')\n",
    "#company_word = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 원문에서 키워드 추출 ( Keybert 사용 )=> 추출 완료 ( keyword.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keybert로 키워드 추출 ( 키워드 1단어들, 2단어들 )\n",
    "summaries = news_raw['summary']\n",
    "# 뉴스 원문 키워드 10개 추출 저장 컬럼\n",
    "news_raw['news_ten_keyword'] = 0\n",
    "\n",
    "for sum_num, summary in enumerate(summaries):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary,top_n=5)\n",
    "    news_raw['keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1)) # 뉴스 원문 키워드 한 단어들 다섯개 추출\n",
    "    news_raw['keyword2'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 2)) # 뉴스 원문 키워드 두 단어들 다섯개 추출\n",
    "    news_raw['ten_keyword'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10) # 뉴스 원문 키워드 10개 추출\n",
    "#news_raw.to_csv('C:/projectnasdaq/project2/keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WordNetLemmatizer 패키지로 단복수 문제 처리\n",
    "news_raw['summary_lemma'] = 0\n",
    "news_raw['tokenize_lemma'] = 0\n",
    "lemma_token = news_raw['tokenize']\n",
    "\n",
    "for i, j in enumerate(lemma_token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_word = [lemmatizer.lemmatize(word) for word in j]\n",
    "    news_raw['tokenize_lemma'][i] = lemma_word\n",
    "    news_raw['summary_lemma'][i] = \" \".join(lemma_word)\n",
    "\n",
    "summary_lemma = news_raw['summary_lemma']\n",
    "for sum_num, summary in enumerate(summary_lemma):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['keyword_lemma'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1))\n",
    "#news_raw.to_csv('C:/projectnasdaq/project2/keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "news_raw['keyword_lemma_ten'] = 0\n",
    "summary_lemma = news_raw['summary_lemma']\n",
    "for sum_num, summary in enumerate(summary_lemma):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(summary)\n",
    "    news_raw['keyword_lemma_ten'][sum_num] = kw_model.extract_keywords(summary, keyphrase_ngram_range=(1, 1),top_n= 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목 정보 수집 ( yfinance 패키지 사용 ) => 수집완료"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 종목코드 리스트\n",
    "stocks = nasdaq_stock['symbol']\n",
    "# 종목코드에 대한 정보 수집용 데이터프레임 생성\n",
    "stock_info = pd.DataFrame(columns=['symbol', 'info'])\n",
    "i = 0\n",
    "\n",
    "for stock_num, stock in enumerate(stocks):\n",
    "    tickers = yf.Ticker(stock)\n",
    "    ticker = tickers.info\n",
    "    i = i + 1\n",
    "    #print(stock_num,stock, \"===>>\",ticker)\n",
    "    stock_info.loc[i] = [stock, ticker]\n",
    "    time.sleep(random.uniform(3, 4))\n",
    "\n",
    "#stock_info.to_csv('C:/projectnasdaq/project2/stock_info.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추출된 회사 이름이랑 ( 전처리 한 ) 뉴스 키워드 연결 ( company_and_keyword.csv 파일에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "company_word = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')\n",
    "keyword = pd.read_csv('C:/projectnasdaq/project2/keyword.csv')\n",
    "\n",
    "# 키워드 전처리\n",
    "keyword['keyword'] = keyword['keyword'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword'] = keyword['keyword'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword'] = keyword['keyword'].str.split(\" \")\n",
    "\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['news_keyword_ten'] = keyword['news_keyword_ten'].str.split(\" \")\n",
    "\n",
    "keyword['keyword2'] = keyword['keyword2'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "keyword['keyword2'] = keyword['keyword2'].str.split(\"  \")\n",
    "# 공백으로 토큰화되거나 비어있는 내용으로 토큰화 되어있으면 지우기\n",
    "keyword['keyword2'] = keyword['keyword2'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword_lemma'] = keyword['keyword_lemma'].str.split(\" \")\n",
    "\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())  # 정규식전처리\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip())  # 공백 여러개 하나로\n",
    "keyword['keyword_lemma_ten'] = keyword['keyword_lemma_ten'].str.split(\" \")\n",
    "\n",
    "keyword['summary_lemma'] = news_raw['summary_lemma']\n",
    "\n",
    "company_word[['keyword', 'keyword2','keyword_lemma']] = 0\n",
    "\n",
    "keyword_list1 = keyword['keyword']\n",
    "keyword_list2 = keyword['keyword2']\n",
    "\n",
    "keyword_id = keyword['news_id']\n",
    "company_id = company_word['news_id']\n",
    "\n",
    "for i, j in enumerate(company_id):\n",
    "    for k, l in enumerate(keyword_id):\n",
    "        if j == l:\n",
    "            company_word['keyword'][i] = keyword['keyword'][k]\n",
    "            company_word['keyword2'][i] = keyword['keyword2'][k]\n",
    "            company_word['keyword_lemma'][i] = keyword['keyword_lemma'][k]\n",
    "\n",
    "#company_word.to_csv('C:/projectnasdaq/project2/company_and_keyword.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yahoo_dataset 파일 수정 ( yahoo_dataset_mapping.csv 에 저장 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# yahoo_dataset 파일 수정 ( -> symbol name 수정 )\n",
    "yahoo_dataset = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset.csv')\n",
    "# yahoo_dataset에서 필요없는 컬럼들 지우고 csv 파일에 있는 종목코드에 종목명 데이터들 붙임\n",
    "\n",
    "yahoo_dataset.insert(1,'new_symbol','0')\n",
    "yahoo_dataset.insert(2,'name','0')\n",
    "yahoo_dataset.insert(3,'company_word','0')\n",
    "\n",
    "yahoo_dataset_symbol = yahoo_dataset['symbol']\n",
    "rep_stock_symbol = rep_stock['symbol']\n",
    "\n",
    "for i, j in enumerate(yahoo_dataset_symbol):\n",
    "    for k, l in enumerate(rep_stock_symbol):\n",
    "        if j == l:\n",
    "            yahoo_dataset['new_symbol'][i] = rep_stock['symbol'][k]\n",
    "            yahoo_dataset['name'][i] = rep_stock['name'][k]\n",
    "            yahoo_dataset['company_word'][i] = rep_stock['name_a'][k]\n",
    "            break\n",
    "\n",
    "# yahoo_dataset 컬럼 정리\n",
    "yahoo_dataset.drop(columns=[\"quoteType\", \"currency\", 'regularMarketPrice', 'regularMarketChange', 'regularMarketChangePercent',\n",
    "             'regularMarketVolume', 'averageDailyVolume3Month', 'marketCap', 'trailingPE', 'fiftyTwoWeekLow',\n",
    "             'fiftyTwoWeekHigh', 'regularMarketOpen', 'priceHint', 'underlyingSymbol'], inplace=True)\n",
    "#yahoo_dataset.to_csv('C:/projectnasdaq/project2/yahoo_dataset_mapping.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 종목명이 추출된 뉴스 키워드들끼리 모으기 ( 딕셔너리 형태로 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "# company단어들 하나씩만 list에 담기 -> 단어 이름이 같은 종목명들의 키워드 모으기 위해서\n",
    "company_and_keyword = pd.read_csv('C:/projectnasdaq/project2/company_and_keyword.csv')\n",
    "company_and_keyword.drop(columns=[\"Unnamed: 0\", 'Unnamed: 0.1'], inplace=True)\n",
    "company_word = company_and_keyword['company_word']\n",
    "\n",
    "company_word_same_list = []\n",
    "for company in company_word:\n",
    "    if company not in company_word_same_list:\n",
    "        company_word_same_list.append(company)\n",
    "\n",
    "company_word_sames = pd.DataFrame(company_word_same_list, columns=['company_word_same'])\n",
    "company_word_same = company_word_sames['company_word_same']\n",
    "\n",
    "# 키워드랑 company_name이랑 데이터프레임에서 맵핑 시키기위한 새로운 dataframe 생성\n",
    "company_name_keyword_organize = pd.DataFrame(columns=['company', 'keyword', 'keyword2','key_list','keyword_lemma'])\n",
    "\n",
    "# 딕셔너리를 { news_id : keyword } 형태로 만들기 위해서 index를 news_id로 설정\n",
    "company_and_keyword = company_and_keyword.set_index(keys='news_id', drop=False, inplace=False)\n",
    "\n",
    "# company_name에 대한 키워드들 딕셔너리 형태로 모으기\n",
    "for i, j in enumerate(company_word_same):\n",
    "    company_name_keyword_organize.loc[i] = [j, company_and_keyword[company_and_keyword['company_word'] == j]['keyword'].to_dict(), company_and_keyword[company_and_keyword['company_word'] == j]['keyword2'].to_dict(),company_and_keyword[company_and_keyword['company_word'] == j]['keyword'].tolist(), company_and_keyword[company_and_keyword['company_word']==j]['keyword_lemma'].tolist()] # 리스트 형태로 키워드만 뽑아오기 위해서"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## news_raw_dataset 만들기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "# company_name_keyword_organize.csv에 column 삽입\n",
    "company_name_keyword_organize.insert(0, 'symbol', '0')\n",
    "company_name_keyword_organize.insert(1, 'name', '0')\n",
    "company_name_keyword_organize.insert(2, 'shortName', '0')\n",
    "company_name_keyword_organize.insert(3, 'longName', '0')\n",
    "company_name_keyword_organize.insert(5, 'company_info', '0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\q1035\\anaconda3\\envs\\intern_lv2\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "yahoo_dataset_mapping = pd.read_csv('C:/projectnasdaq/project2/yahoo_dataset_mapping.csv')\n",
    "company_word_csv = pd.read_csv('C:/projectnasdaq/project2/company_word.csv')\n",
    "\n",
    "# company_name_keyword_organize.csv에 symbol, name 데이터 삽입\n",
    "company_word = company_word_csv['company_word']\n",
    "company = company_name_keyword_organize['company']\n",
    "\n",
    "for i,j in enumerate(company_word):\n",
    "    for k, l in enumerate(company):\n",
    "        if j==l:\n",
    "            company_name_keyword_organize['symbol'][k] = company_word_csv['symbol'][i]\n",
    "            company_name_keyword_organize['name'][k] = company_word_csv['name'][i]\n",
    "            break\n",
    "\n",
    "# company__name.csv에 yahoo finance 정보 수집 내용 삽입\n",
    "yahoo_dataset_mapping_symbol = yahoo_dataset_mapping['new_symbol']\n",
    "company_name_keyword_organize_symbol = company_name_keyword_organize['symbol']\n",
    "\n",
    "for i,j in enumerate(yahoo_dataset_mapping_symbol):\n",
    "    for k,l in enumerate(company_name_keyword_organize_symbol):\n",
    "        if j==l:\n",
    "            company_name_keyword_organize['shortName'][k] = yahoo_dataset_mapping['shortName'][i]\n",
    "            company_name_keyword_organize['longName'][k] = yahoo_dataset_mapping['longName'][i]\n",
    "            company_name_keyword_organize['company_info'][k] = yahoo_dataset_mapping['company_info'][i]\n",
    "            break\n",
    "\n",
    "# 대표 키워드 추출하기\n",
    "# 키워드 빈도수 순으로 정리해서 5개까지만 뽑아오기\n",
    "company_name_keyword_organize['frequency_4'] = 0\n",
    "company_name_keyword_organize['frequency_5'] = 0\n",
    "# lemmatize 용도 컬럼 생성\n",
    "company_name_keyword_organize['frequency_4_lemma'] = 0\n",
    "company_name_keyword_organize['frequency_5_lemma'] = 0\n",
    "\n",
    "\n",
    "key_list_ = company_name_keyword_organize['key_list']\n",
    "for i, j in enumerate(key_list_):\n",
    "    company_name_keyword_organize['key_list'][i] = ''.join(j)\n",
    "\n",
    "keyword_lemma_ = company_name_keyword_organize['keyword_lemma']\n",
    "for i,j in enumerate(keyword_lemma_):\n",
    "    company_name_keyword_organize['keyword_lemma'][i] = ''.join(j)\n",
    "\n",
    "company_name_keyword_organize['key_list'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "company_name_keyword_organize['key_list'].replace(',',' ',regex=True, inplace=True)\n",
    "company_name_keyword_organize['key_list'] = company_name_keyword_organize['key_list'].str.split(\" \")\n",
    "company_name_keyword_organize['key_list'] = company_name_keyword_organize['key_list'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "company_name_keyword_organize['keyword_lemma'].replace(['\\]','\\[','\\''],'',regex=True, inplace=True)\n",
    "company_name_keyword_organize['keyword_lemma'].replace(',',' ',regex=True, inplace=True)\n",
    "company_name_keyword_organize['keyword_lemma'] = company_name_keyword_organize['keyword_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['keyword_lemma'] = company_name_keyword_organize['keyword_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "\n",
    "\n",
    "frequency_word = company_name_keyword_organize['key_list']\n",
    "for i, j in enumerate(frequency_word):\n",
    "    # 대표 키워드 단어 빈도 순으로 4개 추출\n",
    "    company_name_keyword_organize['frequency_4'][i] = Counter(j).most_common(4)\n",
    "    # 대표 키워드 단어 빈도 순으로 5개 추출\n",
    "    company_name_keyword_organize['frequency_5'][i] = Counter(j).most_common(5)\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_4'] = company_name_keyword_organize['frequency_4'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_5'] = company_name_keyword_organize['frequency_5'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# lemmatize 패키지 사용한 단어들로 대표 키워드 빈도 순으로 4-5개 각각 뽑아오기\n",
    "frequency_word_lemma = company_name_keyword_organize['keyword_lemma']\n",
    "for i,j in enumerate(frequency_word_lemma):\n",
    "    company_name_keyword_organize['frequency_4_lemma'][i] = Counter(j).most_common(4)\n",
    "    company_name_keyword_organize['frequency_5_lemma'][i] = Counter(j).most_common(5)\n",
    "\n",
    "# 4개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_4_lemma'] = company_name_keyword_organize['frequency_4_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "# 5개 뽑아온 키워드 전처리\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].apply(lambda x: re.sub('[^a-zA-Z&]', ' ', str(x)).strip())\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].str.split(\" \")\n",
    "company_name_keyword_organize['frequency_5_lemma'] = company_name_keyword_organize['frequency_5_lemma'].apply(lambda x: [i for i in x if i != \"\" and i != \" \"])\n",
    "\n",
    "# 완성된 news_raw dataset\n",
    "#company_name_keyword_organize.to_csv('C:/projectnasdaq/project2/news_raw_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑 (1) 뉴스 원문에 종목명의 대표 키워드가 포함될 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑\n",
    "frequency_4 = company_name_keyword_organize['frequency_4'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "news_stock_mapping = pd.DataFrame(columns=['news_id', 'symbol','name','summary','frequency_4'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(tokenize_list):\n",
    "    for k,l in enumerate(frequency_4):\n",
    "        a = a + 1\n",
    "        #print(news_raw['news_id'][i])\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개 이상 교집합으로 있을 경우\n",
    "        if len(together)==4:\n",
    "            news_stock_mapping.loc[a] = [news_raw['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],news_raw['summary'][i], company_name_keyword_organize['frequency_4'][k]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 맵핑 수 : 1290\n"
     ]
    }
   ],
   "source": [
    "print('총 맵핑 수 :',len(news_stock_mapping))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 맵핑(2) 뉴스 원문 키워드(10개)랑 종목명의 대표 키워드가 일치할 때"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "news_stock_mapping_2 = pd.DataFrame(columns=['news_id', 'symbol','name','summary','frequency_5','news_keyword_ten'])\n",
    "news_raw_keyword_ten = keyword['news_keyword_ten']\n",
    "frequency_5 = company_name_keyword_organize['frequency_5']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_raw_keyword_ten):\n",
    "    for k,l in enumerate(frequency_5):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            news_stock_mapping_2.loc[a] = [keyword['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],keyword['summary'][i], company_name_keyword_organize['frequency_5'][k],keyword['news_keyword_ten'][i]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 맵핑 수 : 1069\n"
     ]
    }
   ],
   "source": [
    "print('총 맵핑 수 :',len(news_stock_mapping_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "news_stock_mapping = news_stock_mapping.drop(columns=['frequency_4'])\n",
    "news_stock_mapping_2 = news_stock_mapping_2.drop(columns=['frequency_5', 'news_keyword_ten'])\n",
    "news_stock_mapping_final = pd.concat([news_stock_mapping,news_stock_mapping_2], ignore_index=True)\n",
    "overlap = news_stock_mapping_final.drop_duplicates(['news_id','symbol'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두가지 방법으로 맵핑했을 때 최종 맵핑 결과 : 1598\n"
     ]
    }
   ],
   "source": [
    "print('두가지 방법으로 맵핑했을 때 최종 맵핑 결과 :' ,len(overlap))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "data": {
      "text/plain": "    symbol                                             name  \\\n0       KO                          Coca-Cola Company (The)   \n1       UL                                     Unilever PLC   \n2      DEO                                       Diageo plc   \n3      BUD  Anheuser-Busch Inbev SA Sponsored ADR (Belgium)   \n4     SBUX                            Starbucks Corporation   \n..     ...                                              ...   \n425    DAL                              Delta Air Lines Inc   \n426   GDRX                              GoodRx Holdings Inc   \n427    SPI                                    SPI Energy Co   \n428    PBR                            Petroleo Brasileiro S   \n429    BAK                                   Braskem SA ADR   \n\n                           shortName                              longName  \\\n0            Coca-Cola Company (The)                 The Coca-Cola Company   \n1                       Unilever PLC                          Unilever PLC   \n2                         Diageo plc                            Diageo plc   \n3    Anheuser-Busch Inbev SA Sponsor            Anheuser-Busch InBev SA/NV   \n4              Starbucks Corporation                 Starbucks Corporation   \n..                               ...                                   ...   \n425            Delta Air Lines, Inc.                 Delta Air Lines, Inc.   \n426            GoodRx Holdings, Inc.                 GoodRx Holdings, Inc.   \n427             SPI Energy Co., Ltd.                  SPI Energy Co., Ltd.   \n428  Petroleo Brasileiro S.A.- Petro  Petróleo Brasileiro S.A. - Petrobras   \n429                       Braskem SA                          Braskem S.A.   \n\n                 company                                       company_info  \\\n0              coca cola  {\"address1\": \"One Coca-Cola Plaza\", \"city\": \"A...   \n1               unilever  {\"address1\": \"Unilever House\", \"address2\": \"10...   \n2                 diageo  {\"address1\": \"Lakeside Drive\", \"address2\": \"Pa...   \n3         anheuser busch  {\"address1\": \"Brouwerijplein 1\", \"city\": \"Leuv...   \n4              starbucks  {\"address1\": \"2401 Utah Avenue South\", \"city\":...   \n..                   ...                                                ...   \n425            delta air  {\"address1\": \"PO Box 20706\", \"city\": \"Atlanta\"...   \n426      goodrx holdings  {\"address1\": \"2701 Olympic Boulevard\", \"addres...   \n427           spi energy  {\"address1\": \"4803 Urbani Avenue\", \"address2\":...   \n428  petroleo brasileiro  {\"address1\": \"Avenida RepUblica do Chile, 65\",...   \n429           braskem sa  {\"address1\": \"120, Rua Lemos Monteiro\", \"addre...   \n\n                                               keyword  \\\n0    {'345585': '['nestle', 'starbucks', 'brands', ...   \n1    {'345585': '['nestle', 'starbucks', 'brands', ...   \n2    {'345585': '['nestle', 'starbucks', 'brands', ...   \n3    {'345585': '['nestle', 'starbucks', 'brands', ...   \n4    {'345585': '['nestle', 'starbucks', 'brands', ...   \n..                                                 ...   \n425  {'443844': '['pandemic', 'airline', 'airlines'...   \n426  {'443702': '['recession', 'stocks', 'stocksthe...   \n427  {'443710': '['tesla', 'ev', 'spi', 'stocks', '...   \n428  {'443728': '['vitol', 'indictment', 'prosecuto...   \n429  {'443728': '['vitol', 'indictment', 'prosecuto...   \n\n                                              keyword2  \\\n0    {'345585': '['nestle sa', 'nestle', 'underperf...   \n1    {'345585': '['nestle sa', 'nestle', 'underperf...   \n2    {'345585': '['nestle sa', 'nestle', 'underperf...   \n3    {'345585': '['nestle sa', 'nestle', 'underperf...   \n4    {'345585': '['nestle sa', 'nestle', 'underperf...   \n..                                                 ...   \n425  {'443844': '['airline inevitable', 'pandemic c...   \n426  {'443702': '['rampant stock', ' stocks slumped...   \n427  {'443710': '['energy stocks', ' spi stock', 'i...   \n428  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n429  {'443728': '['bribes vitol', 'vitol firm', 'au...   \n\n                                              key_list  \\\n0    [nestle, starbucks, brands, bottled, marketpos...   \n1    [nestle, starbucks, brands, bottled, marketpal...   \n2    [nestle, starbucks, brands, bottled, marketdia...   \n3    [nestle, starbucks, brands, bottled, marketinv...   \n4    [nestle, starbucks, brands, bottled, marketdis...   \n..                                                 ...   \n425  [pandemic, airline, airlines, coronavirus, fli...   \n426     [recession, stocks, stocksthe, markets, stock]   \n427                    [tesla, ev, spi, stocks, stock]   \n428  [vitol, indictment, prosecutors, indicted, pro...   \n429  [vitol, indictment, prosecutors, indicted, pro...   \n\n                                         keyword_lemma  \\\n0    [nestle, starbucks, bottled, market, colapostm...   \n1    [nestle, starbucks, bottled, market, coladefor...   \n2    [nestle, starbucks, bottled, market, coladiage...   \n3    [nestle, starbucks, bottled, market, colainves...   \n4    [nestle, starbucks, bottled, market, coladiscr...   \n..                                                 ...   \n425  [vaccine, vaccination, vaccinated, injection, ...   \n426         [argentina, yield, bond, buenos, treasury]   \n427  [mizuho, banking, headquarters, jpmorgan, office]   \n428           [kuwait, kuwaiti, fiscal, debt, deficit]   \n429           [kuwait, kuwaiti, fiscal, debt, deficit]   \n\n                                    frequency_4  \\\n0             [cola, nestle, starbucks, brands]   \n1          [nestle, starbucks, brands, bottled]   \n2          [nestle, starbucks, brands, bottled]   \n3          [nestle, starbucks, brands, bottled]   \n4         [starbucks, shenzhen, nestle, brands]   \n..                                          ...   \n425  [pandemic, airline, airlines, coronavirus]   \n426     [recession, stocks, stocksthe, markets]   \n427                    [tesla, ev, spi, stocks]   \n428  [vitol, indictment, prosecutors, indicted]   \n429  [vitol, indictment, prosecutors, indicted]   \n\n                                           frequency_5  \\\n0           [cola, nestle, starbucks, brands, bottled]   \n1     [nestle, starbucks, brands, bottled, marketpalm]   \n2    [nestle, starbucks, brands, bottled, marketdia...   \n3    [nestle, starbucks, brands, bottled, marketinv...   \n4       [starbucks, shenzhen, nestle, brands, bottled]   \n..                                                 ...   \n425  [pandemic, airline, airlines, coronavirus, fli...   \n426     [recession, stocks, stocksthe, markets, stock]   \n427                    [tesla, ev, spi, stocks, stock]   \n428  [vitol, indictment, prosecutors, indicted, pro...   \n429  [vitol, indictment, prosecutors, indicted, pro...   \n\n                                 frequency_4_lemma  \\\n0             [nestle, starbucks, bottled, market]   \n1             [nestle, starbucks, bottled, market]   \n2            [company, nestle, starbucks, bottled]   \n3             [nestle, starbucks, bottled, market]   \n4         [financing, shenzhen, nestle, starbucks]   \n..                                             ...   \n425  [vaccine, vaccination, vaccinated, injection]   \n426               [argentina, yield, bond, buenos]   \n427      [mizuho, banking, headquarters, jpmorgan]   \n428                [kuwait, kuwaiti, fiscal, debt]   \n429                [kuwait, kuwaiti, fiscal, debt]   \n\n                                     frequency_5_lemma  \n0    [nestle, starbucks, bottled, market, colapostm...  \n1    [nestle, starbucks, bottled, market, coladefor...  \n2        [company, nestle, starbucks, bottled, market]  \n3    [nestle, starbucks, bottled, market, colainves...  \n4    [financing, shenzhen, nestle, starbucks, bottled]  \n..                                                 ...  \n425  [vaccine, vaccination, vaccinated, injection, ...  \n426         [argentina, yield, bond, buenos, treasury]  \n427  [mizuho, banking, headquarters, jpmorgan, office]  \n428           [kuwait, kuwaiti, fiscal, debt, deficit]  \n429           [kuwait, kuwaiti, fiscal, debt, deficit]  \n\n[430 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>shortName</th>\n      <th>longName</th>\n      <th>company</th>\n      <th>company_info</th>\n      <th>keyword</th>\n      <th>keyword2</th>\n      <th>key_list</th>\n      <th>keyword_lemma</th>\n      <th>frequency_4</th>\n      <th>frequency_5</th>\n      <th>frequency_4_lemma</th>\n      <th>frequency_5_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KO</td>\n      <td>Coca-Cola Company (The)</td>\n      <td>Coca-Cola Company (The)</td>\n      <td>The Coca-Cola Company</td>\n      <td>coca cola</td>\n      <td>{\"address1\": \"One Coca-Cola Plaza\", \"city\": \"A...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketpos...</td>\n      <td>[nestle, starbucks, bottled, market, colapostm...</td>\n      <td>[cola, nestle, starbucks, brands]</td>\n      <td>[cola, nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, colapostm...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UL</td>\n      <td>Unilever PLC</td>\n      <td>Unilever PLC</td>\n      <td>Unilever PLC</td>\n      <td>unilever</td>\n      <td>{\"address1\": \"Unilever House\", \"address2\": \"10...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketpal...</td>\n      <td>[nestle, starbucks, bottled, market, coladefor...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketpalm]</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, coladefor...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>Diageo plc</td>\n      <td>Diageo plc</td>\n      <td>diageo</td>\n      <td>{\"address1\": \"Lakeside Drive\", \"address2\": \"Pa...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketdia...</td>\n      <td>[nestle, starbucks, bottled, market, coladiage...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketdia...</td>\n      <td>[company, nestle, starbucks, bottled]</td>\n      <td>[company, nestle, starbucks, bottled, market]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BUD</td>\n      <td>Anheuser-Busch Inbev SA Sponsored ADR (Belgium)</td>\n      <td>Anheuser-Busch Inbev SA Sponsor</td>\n      <td>Anheuser-Busch InBev SA/NV</td>\n      <td>anheuser busch</td>\n      <td>{\"address1\": \"Brouwerijplein 1\", \"city\": \"Leuv...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketinv...</td>\n      <td>[nestle, starbucks, bottled, market, colainves...</td>\n      <td>[nestle, starbucks, brands, bottled]</td>\n      <td>[nestle, starbucks, brands, bottled, marketinv...</td>\n      <td>[nestle, starbucks, bottled, market]</td>\n      <td>[nestle, starbucks, bottled, market, colainves...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SBUX</td>\n      <td>Starbucks Corporation</td>\n      <td>Starbucks Corporation</td>\n      <td>Starbucks Corporation</td>\n      <td>starbucks</td>\n      <td>{\"address1\": \"2401 Utah Avenue South\", \"city\":...</td>\n      <td>{'345585': '['nestle', 'starbucks', 'brands', ...</td>\n      <td>{'345585': '['nestle sa', 'nestle', 'underperf...</td>\n      <td>[nestle, starbucks, brands, bottled, marketdis...</td>\n      <td>[nestle, starbucks, bottled, market, coladiscr...</td>\n      <td>[starbucks, shenzhen, nestle, brands]</td>\n      <td>[starbucks, shenzhen, nestle, brands, bottled]</td>\n      <td>[financing, shenzhen, nestle, starbucks]</td>\n      <td>[financing, shenzhen, nestle, starbucks, bottled]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>DAL</td>\n      <td>Delta Air Lines Inc</td>\n      <td>Delta Air Lines, Inc.</td>\n      <td>Delta Air Lines, Inc.</td>\n      <td>delta air</td>\n      <td>{\"address1\": \"PO Box 20706\", \"city\": \"Atlanta\"...</td>\n      <td>{'443844': '['pandemic', 'airline', 'airlines'...</td>\n      <td>{'443844': '['airline inevitable', 'pandemic c...</td>\n      <td>[pandemic, airline, airlines, coronavirus, fli...</td>\n      <td>[vaccine, vaccination, vaccinated, injection, ...</td>\n      <td>[pandemic, airline, airlines, coronavirus]</td>\n      <td>[pandemic, airline, airlines, coronavirus, fli...</td>\n      <td>[vaccine, vaccination, vaccinated, injection]</td>\n      <td>[vaccine, vaccination, vaccinated, injection, ...</td>\n    </tr>\n    <tr>\n      <th>426</th>\n      <td>GDRX</td>\n      <td>GoodRx Holdings Inc</td>\n      <td>GoodRx Holdings, Inc.</td>\n      <td>GoodRx Holdings, Inc.</td>\n      <td>goodrx holdings</td>\n      <td>{\"address1\": \"2701 Olympic Boulevard\", \"addres...</td>\n      <td>{'443702': '['recession', 'stocks', 'stocksthe...</td>\n      <td>{'443702': '['rampant stock', ' stocks slumped...</td>\n      <td>[recession, stocks, stocksthe, markets, stock]</td>\n      <td>[argentina, yield, bond, buenos, treasury]</td>\n      <td>[recession, stocks, stocksthe, markets]</td>\n      <td>[recession, stocks, stocksthe, markets, stock]</td>\n      <td>[argentina, yield, bond, buenos]</td>\n      <td>[argentina, yield, bond, buenos, treasury]</td>\n    </tr>\n    <tr>\n      <th>427</th>\n      <td>SPI</td>\n      <td>SPI Energy Co</td>\n      <td>SPI Energy Co., Ltd.</td>\n      <td>SPI Energy Co., Ltd.</td>\n      <td>spi energy</td>\n      <td>{\"address1\": \"4803 Urbani Avenue\", \"address2\":...</td>\n      <td>{'443710': '['tesla', 'ev', 'spi', 'stocks', '...</td>\n      <td>{'443710': '['energy stocks', ' spi stock', 'i...</td>\n      <td>[tesla, ev, spi, stocks, stock]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan, office]</td>\n      <td>[tesla, ev, spi, stocks]</td>\n      <td>[tesla, ev, spi, stocks, stock]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan]</td>\n      <td>[mizuho, banking, headquarters, jpmorgan, office]</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>PBR</td>\n      <td>Petroleo Brasileiro S</td>\n      <td>Petroleo Brasileiro S.A.- Petro</td>\n      <td>Petróleo Brasileiro S.A. - Petrobras</td>\n      <td>petroleo brasileiro</td>\n      <td>{\"address1\": \"Avenida RepUblica do Chile, 65\",...</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n      <td>[vitol, indictment, prosecutors, indicted]</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt]</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>BAK</td>\n      <td>Braskem SA ADR</td>\n      <td>Braskem SA</td>\n      <td>Braskem S.A.</td>\n      <td>braskem sa</td>\n      <td>{\"address1\": \"120, Rua Lemos Monteiro\", \"addre...</td>\n      <td>{'443728': '['vitol', 'indictment', 'prosecuto...</td>\n      <td>{'443728': '['bribes vitol', 'vitol firm', 'au...</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n      <td>[vitol, indictment, prosecutors, indicted]</td>\n      <td>[vitol, indictment, prosecutors, indicted, pro...</td>\n      <td>[kuwait, kuwaiti, fiscal, debt]</td>\n      <td>[kuwait, kuwaiti, fiscal, debt, deficit]</td>\n    </tr>\n  </tbody>\n</table>\n<p>430 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name_keyword_organize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문에 대표 키워드가 포함될 경우 508\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 원문에 대표 키워드가 포함될 경우 news_id와 symbol 맵핑(lemmatize)\n",
    "tokenize_list_lemma = news_raw['tokenize_lemma']\n",
    "\n",
    "frequency_5_lemma = company_name_keyword_organize['frequency_5_lemma'] # 키워드 빈도수로 정렬 후 4개만 담아온 리스트\n",
    "news_stock_mapping_lemma = pd.DataFrame(columns=['news_id', 'symbol','name','summary_lemma','frequency_5_lemma'])\n",
    "\n",
    "a = 0\n",
    "for i, j in enumerate(tokenize_list_lemma):\n",
    "    for k,l in enumerate(frequency_5_lemma):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        # 뉴스 원문에 대표 키워드가 4개가 교집합으로 있을 경우\n",
    "        if len(together)==5:\n",
    "            news_stock_mapping_lemma.loc[a] = [news_raw['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],news_raw['summary_lemma'][i], company_name_keyword_organize['frequency_5_lemma'][k]]\n",
    "\n",
    "print('뉴스 원문에 대표 키워드가 포함될 경우',len(news_stock_mapping_lemma))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 : 974\n"
     ]
    }
   ],
   "source": [
    "news_stock_mapping_2_lemma = pd.DataFrame(columns=['news_id', 'symbol','name','summary_lemma','frequency_5_lemma','news_keyword_lemma_ten'])\n",
    "news_raw_keyword_lemma_ten = keyword['keyword_lemma_ten']\n",
    "frequency_5_lemma = company_name_keyword_organize['frequency_5_lemma']\n",
    "# frequency => 종목명의 대표키워드\n",
    "a = 0\n",
    "for i,j in enumerate(news_raw_keyword_lemma_ten):\n",
    "    for k,l in enumerate(frequency_5_lemma):\n",
    "        a = a + 1\n",
    "        together = set(j)&set(l)\n",
    "        if len(together)>=4:\n",
    "            news_stock_mapping_2_lemma.loc[a] = [keyword['news_id'][i], company_name_keyword_organize['symbol'][k], company_name_keyword_organize['name'][k],keyword['summary_lemma'][i], company_name_keyword_organize['frequency_5_lemma'][k],keyword['keyword_lemma_ten'][i]]\n",
    "\n",
    "print('뉴스 원문 키워드와 대표 키워드의 교집합이 4개 이상일 때 :',len(news_stock_mapping_2_lemma))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "data": {
      "text/plain": "     news_id symbol                              name  \\\n0     345585    DEO                        Diageo plc   \n1     345591   ATSG  Air Transport Services Group Inc   \n2     345620   COST      Costco Wholesale Corporation   \n3     345620     DG        Dollar General Corporation   \n4     345620   BBWI             Bath & Body Works Inc   \n...      ...    ...                               ...   \n1477  443843    LEN                Lennar Corporation   \n1478  443844   GDRX               GoodRx Holdings Inc   \n1479  443702    SPI                     SPI Energy Co   \n1480  443718    PBR             Petroleo Brasileiro S   \n1481  443718    BAK                    Braskem SA ADR   \n\n                                          summary_lemma  \n0     nestle sa sailed past ailing consumer good riv...  \n1     nadine scheiner s effort to travel from her ho...  \n2     walmart inc might be one of the few big winner...  \n3     walmart inc might be one of the few big winner...  \n4     walmart inc might be one of the few big winner...  \n...                                                 ...  \n1477  mizuho financial group inc plan to trim office...  \n1478  a sharp recovery in health care dealmaking wil...  \n1479  in a panel discussion during the travel indust...  \n1480  just one week after the u s federal reserve se...  \n1481  just one week after the u s federal reserve se...  \n\n[1262 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>summary_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>345585</td>\n      <td>DEO</td>\n      <td>Diageo plc</td>\n      <td>nestle sa sailed past ailing consumer good riv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>345591</td>\n      <td>ATSG</td>\n      <td>Air Transport Services Group Inc</td>\n      <td>nadine scheiner s effort to travel from her ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345620</td>\n      <td>COST</td>\n      <td>Costco Wholesale Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>345620</td>\n      <td>DG</td>\n      <td>Dollar General Corporation</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>345620</td>\n      <td>BBWI</td>\n      <td>Bath &amp; Body Works Inc</td>\n      <td>walmart inc might be one of the few big winner...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>443843</td>\n      <td>LEN</td>\n      <td>Lennar Corporation</td>\n      <td>mizuho financial group inc plan to trim office...</td>\n    </tr>\n    <tr>\n      <th>1478</th>\n      <td>443844</td>\n      <td>GDRX</td>\n      <td>GoodRx Holdings Inc</td>\n      <td>a sharp recovery in health care dealmaking wil...</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>443702</td>\n      <td>SPI</td>\n      <td>SPI Energy Co</td>\n      <td>in a panel discussion during the travel indust...</td>\n    </tr>\n    <tr>\n      <th>1480</th>\n      <td>443718</td>\n      <td>PBR</td>\n      <td>Petroleo Brasileiro S</td>\n      <td>just one week after the u s federal reserve se...</td>\n    </tr>\n    <tr>\n      <th>1481</th>\n      <td>443718</td>\n      <td>BAK</td>\n      <td>Braskem SA ADR</td>\n      <td>just one week after the u s federal reserve se...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1262 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_stock_mapping_lemma = news_stock_mapping_lemma.drop(columns=['frequency_5_lemma'])\n",
    "news_stock_mapping_2_lemma = news_stock_mapping_2_lemma.drop(columns=['frequency_5_lemma', 'news_keyword_lemma_ten'])\n",
    "news_stock_mapping_final_lemma = pd.concat([news_stock_mapping_lemma,news_stock_mapping_2_lemma], ignore_index=True)\n",
    "overlap = news_stock_mapping_final_lemma.drop_duplicates(['news_id','symbol'])\n",
    "overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}